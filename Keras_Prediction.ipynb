{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "iiyCFgOu99A3",
        "clsPvkLM4kVK",
        "0RuhdgYQ4yN6",
        "HOOGKIi4-gTd"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajbrittle1975/fictional-memory/blob/main/Keras_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check Architechture"
      ],
      "metadata": {
        "id": "iiyCFgOu99A3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %reset"
      ],
      "metadata": {
        "id": "bPW0lMyRaK3w"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8V7WxJu-BA7",
        "outputId": "a32983fb-9fb2-43fb-b12d-15261f6efcc9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Feb  9 19:59:17 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   70C    P0    28W /  70W |    760MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      3729      C                                     757MiB |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Necessary Libraries"
      ],
      "metadata": {
        "id": "clsPvkLM4kVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://launchpad.net/~mario-mariomedina/+archive/ubuntu/talib/+files'\n",
        "ext = '0.4.0-oneiric1_amd64.deb -qO'\n",
        "!wget $url/libta-lib0_$ext libta.deb\n",
        "!wget $url/ta-lib0-dev_$ext ta.deb\n",
        "!dpkg -i libta.deb ta.deb\n",
        "!pip install ta-lib\n",
        "import talib\n",
        "!pip install numpy pandas matplotlib tensorflow tensorflow-datasets keras keras-tuner alpha-vantage"
      ],
      "metadata": {
        "id": "fqmZ5qGE4sCb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0ca0927-4c5f-482b-d3d9-d368bb46e80e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 128074 files and directories currently installed.)\n",
            "Preparing to unpack libta.deb ...\n",
            "Unpacking libta-lib0 (0.4.0-oneiric1) over (0.4.0-oneiric1) ...\n",
            "Preparing to unpack ta.deb ...\n",
            "Unpacking ta-lib0-dev (0.4.0-oneiric1) over (0.4.0-oneiric1) ...\n",
            "Setting up libta-lib0 (0.4.0-oneiric1) ...\n",
            "Setting up ta-lib0-dev (0.4.0-oneiric1) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ta-lib in /usr/local/lib/python3.8/dist-packages (0.4.25)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from ta-lib) (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.3.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (3.2.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (2.9.2)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.8/dist-packages (4.8.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.8/dist-packages (2.9.0)\n",
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.8/dist-packages (1.2.0)\n",
            "Requirement already satisfied: alpha-vantage in /usr/local/lib/python3.8/dist-packages (2.3.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.30.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (4.4.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow) (23.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.51.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (15.0.6.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (1.12.0)\n",
            "Requirement already satisfied: etils[enp,epath]>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (1.0.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (2.3)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (0.1.8)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (0.10.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (4.64.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (2.25.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (5.4.8)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (5.10.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (0.3.6)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.8/dist-packages (from keras-tuner) (1.0.4)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.8/dist-packages (from keras-tuner) (7.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from alpha-vantage) (3.8.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.8/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets) (3.12.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow-datasets) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow-datasets) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2022.12.7)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.16.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->alpha-vantage) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->alpha-vantage) (22.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->alpha-vantage) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->alpha-vantage) (1.8.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->alpha-vantage) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->alpha-vantage) (2.1.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->alpha-vantage) (1.3.3)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (0.2.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (2.0.10)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (0.7.5)\n",
            "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (0.18.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (4.4.2)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (5.7.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-metadata->tensorflow-datasets) (1.58.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.10->ipython->keras-tuner) (0.8.3)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (6.0.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->keras-tuner) (0.2.6)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect->ipython->keras-tuner) (0.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "0RuhdgYQ4yN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "from numpy import array\n",
        "from numpy import hstack\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from pandas import DataFrame as df\n",
        "from pandas import concat\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from keras import layers\n",
        "import keras_tuner as kt\n",
        "from keras.models import Sequential \n",
        "from keras.layers import Activation\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import GRU\n",
        "from keras.utils import timeseries_dataset_from_array\n",
        "\n",
        "from talib import abstract\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "from alpha_vantage.timeseries import TimeSeries\n",
        "from alpha_vantage.techindicators import TechIndicators\n",
        "\n",
        "print(\"All libraries loaded\")"
      ],
      "metadata": {
        "id": "_b0MGjwj41Tx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e263955b-a689-42ce-d21f-327a05d8ddeb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All libraries loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stock Configurations"
      ],
      "metadata": {
        "id": "uAu1wLWN9tyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"alpha_vantage\": {\n",
        "        # Claim your free API key here: https://www.alphavantage.co/support/#api-key\n",
        "        \"key\": \"T7GH3GWEIV1DKHB0\",\n",
        "        \"symbol\": \"TQQQ\",\n",
        "        \"outputsize\": \"full\",\n",
        "        \"key_adjusted_close\": \"5. adjusted close\",\n",
        "        \"key_high\": \"2. high\",\n",
        "        \"key_low\": \"3. low\",\n",
        "        \"key_volume\": \"6. volume\",\n",
        "    },\n",
        "    \"data\": {\n",
        "        \"window_size\": 25,\n",
        "        \"train_split_size\": 0.80,\n",
        "        \"dynamic_calcs\": False,\n",
        "    },\n",
        "    \"plots\": {\n",
        "        \"xticks_interval\": 90,  # show a date every 90 days\n",
        "        \"color_actual\": \"#001f3f\",\n",
        "        \"color_train\": \"#3D9970\",\n",
        "        \"color_val\": \"#0074D9\",\n",
        "        \"color_pred_train\": \"#3D9970\",\n",
        "        \"color_pred_val\": \"#0074D9\",\n",
        "        \"color_pred_test\": \"#FF4136\",\n",
        "    },\n",
        "    \"model\": {\n",
        "        \"input_size\": 11,  # we are using 11 features\n",
        "        \"num_lstm_layers\": 2,\n",
        "        \"lstm_size\": 120,\n",
        "        \"dropout\": 0.20,\n",
        "    },\n",
        "    \"training\": {\n",
        "        \"device\": \"cuda\",  # \"cuda\" or \"cpu\"\n",
        "        \"batch_size\": 25,\n",
        "        \"num_epoch\": 200,\n",
        "        \"learning_rate\": 0.01,\n",
        "        \"scheduler_step_size\": 40,\n",
        "    },\n",
        "}\n"
      ],
      "metadata": {
        "id": "F_g-F9jY9nQm"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Necessary Functions"
      ],
      "metadata": {
        "id": "cb3ocO-Q-OxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_nan_columns(data_array, data_date):\n",
        "    # return data_array[~np.isnan(data_array).any(axis=1), :]\n",
        "    nan_rows = np.argwhere(np.isnan(data_array).any(axis=1))\n",
        "    print(\"Removing {0} nan rows from data stack...\".format(len(nan_rows)))\n",
        "    data_array = np.delete(data_array, nan_rows, 0)\n",
        "\n",
        "    # given index of elements\n",
        "    # remove largest indices first to not change length\n",
        "    nan_row_list = list(nan_rows.flatten())\n",
        "    data_date = list(data_date)\n",
        "    for ele in sorted(nan_row_list, reverse=True):\n",
        "        data_date.remove(data_date[ele])\n",
        "\n",
        "    return data_array, data_date\n",
        "\n",
        "\n",
        "class Normalizer:\n",
        "    def __init__(self):\n",
        "        self.mu = None\n",
        "        self.sd = None\n",
        "\n",
        "    def fit_transform(self, x):\n",
        "        self.mu = np.nanmean(x, axis=(0), keepdims=True)\n",
        "        self.sd = np.nanstd(x, axis=(0), keepdims=True)\n",
        "        normalized_x = (x - self.mu) / self.sd\n",
        "        return normalized_x\n",
        "\n",
        "    def inverse_transform(self, x):\n",
        "        return (x * self.sd) + self.mu\n",
        "\n",
        "\n",
        "def download_data(config):\n",
        "    ts = TimeSeries(key=config[\"alpha_vantage\"][\"key\"])\n",
        "    ti = TechIndicators(key=config[\"alpha_vantage\"][\"key\"])\n",
        "    dynamic_calcs = config[\"data\"][\"dynamic_calcs\"] ## if we want to use dynamic calculation of indicators this is true\n",
        "\n",
        "    print(\"Obtaining data for symbol: {0}\".format(config[\"alpha_vantage\"][\"symbol\"]))\n",
        "    data, data_meta_data = ts.get_daily_adjusted(\n",
        "        config[\"alpha_vantage\"][\"symbol\"],\n",
        "        outputsize=config[\"alpha_vantage\"][\"outputsize\"],\n",
        "    )\n",
        "\n",
        "    print(\"Sorting data...\")\n",
        "\n",
        "    data_date = [date for date in data.keys()]\n",
        "    data_date.reverse()\n",
        "\n",
        "    print(\"Sorting close price...\")\n",
        "    data_close_price = [\n",
        "        float(data[date][config[\"alpha_vantage\"][\"key_adjusted_close\"]])\n",
        "        for date in data.keys()\n",
        "    ]\n",
        "    data_close_price.reverse()\n",
        "    data_close_price = np.array(data_close_price)\n",
        "    data_price_pct_change = np.diff(data_close_price) / data_close_price[:-1] * 100\n",
        "    data_price_pct_change = np.pad(data_price_pct_change, (1, 0), \"constant\")\n",
        "    data_price_pct_change = data_price_pct_change.reshape(-1, 1)\n",
        "    ## Normalize price percent change\n",
        "    print(\"Normalizing price percent change data...\")\n",
        "    price_pct_change_scaler = MinMaxScaler()\n",
        "    data_price_pct_change_norm = price_pct_change_scaler.fit_transform(\n",
        "        data_price_pct_change\n",
        "    )\n",
        "\n",
        "    print(\"Sorting high prices...\")\n",
        "    data_high_price = [\n",
        "        float(data[date][config[\"alpha_vantage\"][\"key_high\"]]) for date in data.keys()\n",
        "    ]\n",
        "    data_high_price.reverse()\n",
        "    data_high_price = np.array(data_high_price)\n",
        "\n",
        "    print(\"Sorting low prices...\")\n",
        "    data_low_price = [\n",
        "        float(data[date][config[\"alpha_vantage\"][\"key_low\"]]) for date in data.keys()\n",
        "    ]\n",
        "    data_low_price.reverse()\n",
        "    data_low_price = np.array(data_low_price)\n",
        "\n",
        "    print(\"Sorting volume...\")\n",
        "    data_volume = [\n",
        "        float(data[date][config[\"alpha_vantage\"][\"key_volume\"]]) for date in data.keys()\n",
        "    ]\n",
        "    data_volume.reverse()\n",
        "    data_volume = np.array(data_volume)\n",
        "    # Calculate OBV before reshaping data\n",
        "    print(\"Calculating OBV on balance volume...\")\n",
        "    obv_data = abstract.OBV(data_close_price, data_volume)\n",
        "    data_volume = data_volume.reshape(-1, 1)\n",
        "    ## Normalize Volume\n",
        "    print(\"Normalizing Volume Data...\")\n",
        "    volume_scaler = Normalizer()\n",
        "    volume_norm = volume_scaler.fit_transform(data_volume)\n",
        "\n",
        "    print(\"Calculating HT_TRENDLINE...\")\n",
        "    ## Get HT Trend data\n",
        "    ht_trend = abstract.HT_TRENDLINE(data_close_price)\n",
        "    ht_trend = ht_trend.reshape(-1, 1)\n",
        "\n",
        "    ## Normalize HT Trend\n",
        "    print(\"Normalizing HT_TRENDLINE data...\")\n",
        "    ht_trendline_scaler = MinMaxScaler()\n",
        "    ht_trendline_norm = ht_trendline_scaler.fit_transform(ht_trend)\n",
        "    \n",
        "    print(\"Normalizing On Balance Volume data...\")\n",
        "    ## Normalize OBV\n",
        "    obv_norm = obv_data / data_volume\n",
        "    obv_data = obv_data.reshape(-1, 1)\n",
        "    obv_scaler = Normalizer()\n",
        "    obv_norm = obv_scaler.fit_transform(obv_data)  \n",
        "\n",
        "    print(\"Calculating MESA average price...\")\n",
        "    ## Get MESA average price\n",
        "    mama, fama = abstract.MAMA(data_close_price, fastlimit=0.5, slowlimit=0.05)\n",
        "    mesa_hist = mama - fama\n",
        "    mesa_hist = mesa_hist.reshape(-1, 1)\n",
        "    ## Normalize MESA\n",
        "    print(\"Normalizing dynamic MESA trend data...\")\n",
        "    mesa_hist_scaler = Normalizer()\n",
        "    mesa_hist_norm = mesa_hist_scaler.fit_transform(mesa_hist)\n",
        "\n",
        "    print(\"Calculating HT_TREND sine signals...\")\n",
        "    ## Get HT_SINE signals\n",
        "    sine, leadsine = abstract.HT_SINE(data_close_price)\n",
        "    trend = abstract.HT_TRENDMODE(data_close_price)\n",
        "    ht_trend_sine = (sine - leadsine) * trend\n",
        "    ht_trend_sine = ht_trend_sine.reshape(-1, 1)\n",
        "    ## Normalize HT_TREND\n",
        "    print(\"Normalizing dynamic MACD histogram data...\")\n",
        "    ht_trend_sine_scaler = Normalizer()\n",
        "    ht_trend_sine_norm = ht_trend_sine_scaler.fit_transform(ht_trend_sine)\n",
        "\n",
        "    print(\"Calculating HT_PERIODS for dynamic indicators...\")\n",
        "    # Get period of data close price\n",
        "    ht_period = abstract.HT_DCPERIOD(data_close_price)\n",
        "    ht_period = np.around(ht_period)  # the instant period is optimal for oscillators\n",
        "\n",
        "    print(\"Provisioning arrays for dynamic indicators...\")\n",
        "    # Provision arrays for additional indicators\n",
        "    num_data_points = len(data_date)\n",
        "    adx_ht = np.zeros(num_data_points, dtype=float)\n",
        "    aroon_osc_ht = np.zeros(num_data_points, dtype=float)\n",
        "    rsi_ht = np.zeros(num_data_points, dtype=float)\n",
        "    macd_ht = np.zeros(num_data_points, dtype=float)\n",
        "    macdsignal_ht = np.zeros(num_data_points, dtype=float)\n",
        "    macdhist_ht = np.zeros(num_data_points, dtype=float)\n",
        "    slowk_ht = np.zeros(num_data_points, dtype=float)\n",
        "    slowd_ht = np.zeros(num_data_points, dtype=float)\n",
        "    bb_lowerband_ht = np.zeros(num_data_points, dtype=float)\n",
        "    bb_upperband_ht = np.zeros(num_data_points, dtype=float)\n",
        "    bb_pct_ht = np.zeros(num_data_points, dtype=float)\n",
        "    ult_osc_ht = np.zeros(num_data_points, dtype=float)\n",
        "\n",
        "    # Get frequency-adjusted ADX\n",
        "    print(\"Calculating dynamic ADX...\")\n",
        "    if dynamic_calcs:\n",
        "      i = 0\n",
        "      for period in ht_period:\n",
        "          if not math.isnan(period):\n",
        "              period = int(period)\n",
        "              slowperiod = (\n",
        "                  period if period % 2 == 0 else period + 1\n",
        "              )  # Ensure slow period is even\n",
        "              adx = abstract.ADX(\n",
        "                  data_high_price, data_low_price, data_close_price, timeperiod=slowperiod\n",
        "              )\n",
        "              adx_ht[i] = adx[i]\n",
        "\n",
        "          i += 1\n",
        "      else:\n",
        "        adx_ht = abstract.ADX(\n",
        "          data_high_price, data_low_price, data_close_price\n",
        "          )\n",
        "\n",
        "    adx_ht = adx_ht.reshape(-1, 1)\n",
        "    ## Normalize ADX\n",
        "    print(\"Normalizing dynamic ADX data...\")\n",
        "    adx_norm = adx_ht / 100\n",
        "\n",
        "    # Get frequency-adjusted Aroon Oscillator\n",
        "    print(\"Calculating dynamic AROONOSC...\")\n",
        "    if dynamic_calcs:\n",
        "      i = 0\n",
        "      for period in ht_period:\n",
        "          if not math.isnan(period):\n",
        "              period = int(period)\n",
        "              slowperiod = (\n",
        "                  period if period % 2 == 0 else period + 1\n",
        "              )  # Ensure slow period is even\n",
        "              aroon_osc = abstract.AROONOSC(\n",
        "                  data_high_price, data_low_price, timeperiod=int(slowperiod/2)\n",
        "              )\n",
        "              aroon_osc_ht[i] = aroon_osc[i]\n",
        "\n",
        "          i += 1\n",
        "      else:\n",
        "        aroon_osc_ht = abstract.AROONOSC(\n",
        "            data_high_price, data_low_price\n",
        "        )\n",
        "\n",
        "    aroon_osc_ht = aroon_osc_ht.reshape(-1, 1)\n",
        "    ## Normalize Aroon Oscillator\n",
        "    print(\"Normalizing dynamic AROONOSC data...\")\n",
        "    aroon_osc_norm = (aroon_osc_ht + 100) / 200  # oscillates between +/- 100\n",
        "\n",
        "    # Get frequency-adjusted MACD\n",
        "    print(\"Calculating dynamic MACD...\")\n",
        "    if dynamic_calcs:\n",
        "      i = 0\n",
        "      for period in ht_period:\n",
        "          if not math.isnan(period):\n",
        "              period = int(period)\n",
        "              slowperiod = (\n",
        "                  period if period % 2 == 0 else period + 1\n",
        "              )  # Ensure slow period is even\n",
        "              fastperiod = int((slowperiod / 2) - 1)\n",
        "              signalperiod = math.floor(0.75 * fastperiod)\n",
        "              macd, macdsignal, macdhist = abstract.MACD(\n",
        "                  data_close_price,\n",
        "                  fastperiod=fastperiod,\n",
        "                  slowperiod=slowperiod,\n",
        "                  signalperiod=signalperiod,\n",
        "              )\n",
        "              macd_ht[i] = macd[i]\n",
        "              macdsignal_ht[i] = macdsignal[i]\n",
        "              macdhist_ht[i] = macdhist[i]\n",
        "\n",
        "          i += 1\n",
        "\n",
        "      else:\n",
        "        macd_ht, macdsignal_ht, macdhist_ht = abstract.MACD(\n",
        "            data_close_price\n",
        "        )\n",
        "\n",
        "\n",
        "    macdhist_ht = macdhist_ht.reshape(-1, 1)\n",
        "    ## Normalize MACD\n",
        "    print(\"Normalizing dynamic MACD histogram data...\")\n",
        "    macd_scaler = Normalizer()\n",
        "    macd_norm = macd_scaler.fit_transform(macdhist_ht)\n",
        "\n",
        "    # Get frequency-adjusted Bollinger Bands\n",
        "    print(\"Calculating dynamic BBANDS...\")\n",
        "    if dynamic_calcs:\n",
        "      i = 0\n",
        "      for period in ht_period:\n",
        "          if not math.isnan(period):\n",
        "              period = int(period)\n",
        "              slowperiod = (\n",
        "                  period if period % 2 == 0 else period + 1\n",
        "              )  # Ensure slow period is even\n",
        "              bb_upperband, bb_middleband, bb_lowerband = abstract.BBANDS(\n",
        "                  data_close_price,\n",
        "                  timeperiod=slowperiod,\n",
        "                  nbdevup=float(2),\n",
        "                  nbdevdn=float(2),\n",
        "                  matype=1,\n",
        "              )\n",
        "              bb_upperband_ht[i] = bb_upperband[i]\n",
        "              bb_lowerband_ht[i] = bb_lowerband[i]\n",
        "              bb_pct_ht[i] = (data_close_price[i] - bb_lowerband[i]) / (\n",
        "                  bb_upperband[i] - bb_lowerband[i]\n",
        "              )\n",
        "\n",
        "          i += 1\n",
        "\n",
        "      else:\n",
        "        bb_upperband, bb_middleband, bb_lowerband = abstract.BBANDS(\n",
        "            data_close_price\n",
        "        )\n",
        "        bb_pct_ht = (data_close_price - bb_lowerband) / (\n",
        "            bb_upperband - bb_lowerband\n",
        "        )\n",
        "\n",
        "    bb_pct_ht = bb_pct_ht.reshape(-1, 1)\n",
        "    ## Normalize MACD\n",
        "    print(\"Normalizing dynamic BBANDS Percentage data...\")\n",
        "    bb_pct_scaler = Normalizer()\n",
        "    bb_pct_norm = bb_pct_scaler.fit_transform(bb_pct_ht)\n",
        "\n",
        "    # Get frequency-adjusted RSI\n",
        "    print(\"Calculating dynamic RSI...\")\n",
        "    if dynamic_calcs:\n",
        "      i = 0\n",
        "      for period in ht_period:\n",
        "          if not math.isnan(period):\n",
        "              period = (\n",
        "                  period / 2 if period % 2 == 0 else (period + 1) / 2\n",
        "              )  # Ensure slow period is even and divide by 2\n",
        "              period = int(period)\n",
        "              rsi = abstract.RSI(\n",
        "                  data_close_price,\n",
        "                  timeperiod=period,\n",
        "              )\n",
        "              rsi_ht[i] = rsi[i]\n",
        "\n",
        "          i += 1\n",
        "      else:\n",
        "          rsi_ht = abstract.RSI(\n",
        "                  data_close_price,\n",
        "              )\n",
        "        \n",
        "\n",
        "    rsi_ht = rsi_ht.reshape(-1, 1)\n",
        "    ## Normalize RSI\n",
        "    print(\"Normalizing dynamic RSI data...\")\n",
        "    rsi_norm = rsi_ht / 100\n",
        "\n",
        "    # Get frequency-adjusted Stochastic\n",
        "    print(\"Calculating dynamic Stochastic...\")\n",
        "    if dynamic_calcs:\n",
        "      i = 0\n",
        "      for period in ht_period:\n",
        "          if not math.isnan(period):\n",
        "              period = (\n",
        "                  period / 2 if period % 2 == 0 else (period + 1) / 2\n",
        "              )  # Ensure slow period is even and divide by 2\n",
        "              period = int(period)\n",
        "              slowk, slowd = abstract.STOCH(\n",
        "                  data_high_price,\n",
        "                  data_low_price,\n",
        "                  data_close_price,\n",
        "                  fastk_period=period,\n",
        "                  slowk_period=3,\n",
        "                  slowk_matype=1,\n",
        "                  slowd_period=3,\n",
        "                  slowd_matype=1,\n",
        "              )\n",
        "              slowk_ht[i] = slowk[i]\n",
        "              slowd_ht[i] = slowd[i]\n",
        "\n",
        "          i += 1\n",
        "      else:\n",
        "        slowk_ht, slowd_ht = abstract.STOCH(\n",
        "          data_high_price,\n",
        "          data_low_price,\n",
        "          data_close_price,\n",
        "        )\n",
        "\n",
        "    stochastic_hist_ht = slowk_ht - slowd_ht\n",
        "    stochastic_hist_ht = stochastic_hist_ht.reshape(-1, 1)\n",
        "    ## Normalize dynamic stochastic histogram\n",
        "    print(\"Normalizing dynamic stochastic histogram data...\")\n",
        "    stochastic_hist_scaler = Normalizer()\n",
        "    stochastic_hist_norm = stochastic_hist_scaler.fit_transform(stochastic_hist_ht)\n",
        "    \n",
        "    # Get frequency-adjusted Ultimate Oscillator\n",
        "    print(\"Calculating dynamic ULTOSC...\")\n",
        "    if dynamic_calcs:\n",
        "      i = 0\n",
        "      for period in ht_period:\n",
        "          if not math.isnan(period):\n",
        "              period = (\n",
        "                  period / 2 if period % 2 == 0 else (period + 1) / 2\n",
        "              )  # Ensure slow period is even and divide by 2\n",
        "              period = int(period)\n",
        "              ult_osc = abstract.ULTOSC(\n",
        "                  data_high_price,\n",
        "                  data_low_price,\n",
        "                  data_close_price,\n",
        "                  timeperiod1=period,\n",
        "                  timeperiod2=period*2,\n",
        "                  timeperiod3=period*4,\n",
        "              )\n",
        "              ult_osc_ht[i] = ult_osc[i]\n",
        "\n",
        "          i += 1\n",
        "      else:\n",
        "        ult_osc_ht = abstract.ULTOSC(\n",
        "            data_high_price,\n",
        "            data_low_price,\n",
        "            data_close_price,\n",
        "        )\n",
        "\n",
        "    ult_osc_ht = ult_osc_ht.reshape(-1, 1)\n",
        "    ## Normalize dynamic stochastic histogram\n",
        "    print(\"Normalizing dynamic ULTOSC data...\")\n",
        "    \n",
        "\n",
        "    print(\"volume_norm: {0}\".format(volume_norm.shape))\n",
        "    print(\"obv_norm: {0}\".format(obv_norm.shape))\n",
        "    print(\"mesa_hist_norm: {0}\".format(mesa_hist_norm.shape))\n",
        "    print(\"ht_trend_sine: {0}\".format(ht_trend_sine.shape))\n",
        "    print(\"macd_norm: {0}\".format(volume_norm.shape))\n",
        "    print(\"adx_norm: {0}\".format(adx_norm.shape))\n",
        "    print(\"aroon_osc_norm: {0}\".format(aroon_osc_norm.shape))\n",
        "    print(\"bb_pct_norm: {0}\".format(bb_pct_norm.shape))\n",
        "    print(\"rsi_norm: {0}\".format(rsi_norm.shape))\n",
        "    print(\"ult_osc_ht: {0}\".format(ult_osc_ht.shape))\n",
        "    print(\"data_price_pct_change_norm: {0}\".format(data_price_pct_change_norm.shape))\n",
        "\n",
        "\n",
        "\n",
        "    data_stack = hstack(\n",
        "        (\n",
        "            volume_norm,\n",
        "            obv_norm,\n",
        "            mesa_hist_norm,\n",
        "            ht_trend_sine,\n",
        "            macd_norm,\n",
        "            adx_norm,\n",
        "            aroon_osc_norm,\n",
        "            bb_pct_norm,\n",
        "            rsi_norm,\n",
        "            stochastic_hist_norm,\n",
        "            ult_osc_ht,\n",
        "            data_price_pct_change_norm,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    print(data_stack)\n",
        "    # print(data_stack.size)\n",
        "    data_stack, data_date = remove_nan_columns(data_stack, data_date)\n",
        "    num_data_points = data_stack.shape[0]\n",
        "    display_date_range = (\n",
        "        \"from \" + data_date[0] + \" to \" + data_date[num_data_points - 1]\n",
        "    )\n",
        "    print(\"Number data points:\", num_data_points, display_date_range)\n",
        "\n",
        "    # print(data_stack)\n",
        "    # print(data_stack.size)\n",
        "\n",
        "    return data_date, data_stack, num_data_points, display_date_range, price_pct_change_scaler"
      ],
      "metadata": {
        "id": "rZs_-wGu-SmC"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Stock Data"
      ],
      "metadata": {
        "id": "HOOGKIi4-gTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_date, data_stack, num_data_points, display_date_range, price_pct_change_scaler = download_data(config)\n",
        "\n",
        "# # Convert date data into a datetime object and pandas column\n",
        "# df_date = pd.DataFrame(data_date, columns=['Date'])\n",
        "# df_date = pd.to_datetime(df_date['Date'])\n",
        "\n",
        "# # Create full pandas datastack indexed by date\n",
        "# df = pd.DataFrame(\n",
        "#             data_stack, \n",
        "#             columns = [  \n",
        "#             \"mesa_hist\",          \n",
        "#             \"ht_trend_sine\",\n",
        "#             \"macd_norm\",\n",
        "#             \"adx_norm\",\n",
        "#             \"aroon_osc_norm\",\n",
        "#             \"bb_pct_norm\",\n",
        "#             \"rsi_norm\",\n",
        "#             \"stochastic_hist_norm\",\n",
        "#             \"ult_osc_ht\",\n",
        "#             \"data_price_pct_change_norm\"], index = df_date\n",
        "#             )\n"
      ],
      "metadata": {
        "id": "tG_3OUkj-iq5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "364a06d2-1105-4fd1-b5c8-a0714052f920"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining data for symbol: TQQQ\n",
            "Sorting data...\n",
            "Sorting close price...\n",
            "Normalizing price percent change data...\n",
            "Sorting high prices...\n",
            "Sorting low prices...\n",
            "Sorting volume...\n",
            "Calculating OBV on balance volume...\n",
            "Normalizing Volume Data...\n",
            "Calculating HT_TRENDLINE...\n",
            "Normalizing HT_TRENDLINE data...\n",
            "Normalizing On Balance Volume data...\n",
            "Calculating MESA average price...\n",
            "Normalizing dynamic MESA trend data...\n",
            "Calculating HT_TREND sine signals...\n",
            "Normalizing dynamic MACD histogram data...\n",
            "Calculating HT_PERIODS for dynamic indicators...\n",
            "Provisioning arrays for dynamic indicators...\n",
            "Calculating dynamic ADX...\n",
            "Normalizing dynamic ADX data...\n",
            "Calculating dynamic AROONOSC...\n",
            "Normalizing dynamic AROONOSC data...\n",
            "Calculating dynamic MACD...\n",
            "Normalizing dynamic MACD histogram data...\n",
            "Calculating dynamic BBANDS...\n",
            "Normalizing dynamic BBANDS Percentage data...\n",
            "Calculating dynamic RSI...\n",
            "Normalizing dynamic RSI data...\n",
            "Calculating dynamic Stochastic...\n",
            "Normalizing dynamic stochastic histogram data...\n",
            "Calculating dynamic ULTOSC...\n",
            "Normalizing dynamic ULTOSC data...\n",
            "volume_norm: (3271, 1)\n",
            "obv_norm: (3271, 1)\n",
            "mesa_hist_norm: (3271, 1)\n",
            "ht_trend_sine: (3271, 1)\n",
            "macd_norm: (3271, 1)\n",
            "adx_norm: (3271, 1)\n",
            "aroon_osc_norm: (3271, 1)\n",
            "bb_pct_norm: (3271, 1)\n",
            "rsi_norm: (3271, 1)\n",
            "ult_osc_ht: (3271, 1)\n",
            "data_price_pct_change_norm: (3271, 1)\n",
            "[[-5.13688152e-01 -5.01753142e-01             nan ...  4.98124776e-02\n",
            "   0.00000000e+00  5.60832801e-01]\n",
            " [-5.05023397e-01 -5.00688062e-01             nan ...  4.98124776e-02\n",
            "   0.00000000e+00  5.67494534e-01]\n",
            " [-5.03309845e-01 -4.99496978e-01             nan ...  4.98124776e-02\n",
            "   0.00000000e+00  6.23862050e-01]\n",
            " ...\n",
            " [ 2.75030978e+00 -1.99372534e+00  2.03061853e+00 ... -3.90260124e-02\n",
            "   7.78415853e+01  5.19386505e-01]\n",
            " [ 4.73217924e+00 -1.60755242e+00  1.92681777e+00 ...  5.49277815e-02\n",
            "   8.30583349e+01  6.61595078e-01]\n",
            " [ 3.14315350e+00 -1.87687932e+00  1.55707881e+00 ...  2.80050593e-03\n",
            "   7.76622002e+01  4.73339867e-01]]\n",
            "Removing 67 nan rows from data stack...\n",
            "Number data points: 3204 from 2010-05-19 to 2023-02-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split Data Into Train and Test Groups"
      ],
      "metadata": {
        "id": "JLcfFoXtH__R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters from config\n",
        "batch_size = config[\"training\"][\"batch_size\"]\n",
        "epochs = config[\"training\"][\"num_epoch\"]\n",
        "dropout=config[\"model\"][\"dropout\"]\n",
        "window_size = config[\"data\"][\"window_size\"]\n",
        "features = config[\"model\"][\"input_size\"]\n",
        "neuron_per_feature = 8\n",
        "\n",
        "# split into train and test sets\n",
        "input_data = data_stack[:, :-1] # all but last column\n",
        "target_data = data_stack[:, -1] # for last column\n",
        "print(target_data)\n",
        "target_data = np.pad(target_data,((1,0)), mode='constant')[:-1] # Shift so we are getting target for next day\n",
        "target_data = np.reshape(target_data, (input_data.shape[0], 1) )\n",
        "print(target_data)\n",
        "\n",
        "dataset = keras.utils.timeseries_dataset_from_array(\n",
        "    input_data, \n",
        "    target_data, \n",
        "    sequence_length=window_size, \n",
        "    batch_size=batch_size,\n",
        "    seed=None)\n",
        "\n",
        "batch_count = int(input_data.shape[0]/batch_size)"
      ],
      "metadata": {
        "id": "NL9ozoDwFsbH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9768280-1b4e-4450-c763-6bc9f5e4fb42"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.51788744 0.39623752 0.57343519 ... 0.5193865  0.66159508 0.47333987]\n",
            "[[0.        ]\n",
            " [0.51788744]\n",
            " [0.39623752]\n",
            " ...\n",
            " [0.47677962]\n",
            " [0.5193865 ]\n",
            " [0.66159508]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### GRU Network"
      ],
      "metadata": {
        "id": "ES-fPK5tAeYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(hp):\n",
        "  dnn_units_min, dnn_units_max = 32, 512\n",
        "  active_func_ss = ['relu', 'tanh']\n",
        "  optimizer_ss = ['adam']\n",
        "  lr_min, lr_max = 1e-4, 1e-1\n",
        "  \n",
        "  active_func = hp.Choice('activation', active_func_ss)\n",
        "  optimizer = hp.Choice('optimizer', optimizer_ss)\n",
        "  lr = hp.Float('learning_rate', min_value=lr_min, max_value=lr_max, sampling='log')\n",
        "  dropout_ss = hp.Choice(\"dropouts_\", [0.1, 0.2, 0.3, 0.4, 0.5])\n",
        "    \n",
        "  # Build Model\n",
        "  # Sequential\n",
        "  model = tf.keras.Sequential()\n",
        "\n",
        "  # First GRU Layer\n",
        "  # input shape should be form (batch, window_size, features)\n",
        "  npf1 = hp.Int(\n",
        "      name='neuron_per_feature1',\n",
        "      min_value=2,\n",
        "      max_value=200,\n",
        "      step=4,\n",
        "      sampling=None,\n",
        "      default=None,\n",
        "      parent_name=None,\n",
        "      parent_values=None,\n",
        "  )\n",
        "  model.add(tf.keras.layers.GRU(\n",
        "      npf1, \n",
        "      input_shape=(window_size, features),\n",
        "      activation = 'tanh',\n",
        "      dropout=dropout_ss,\n",
        "      return_sequences=True,\n",
        "      name='gru1',)\n",
        "      )\n",
        "  \n",
        "  # Second GRU Layer\n",
        "  npf2 = hp.Int(\n",
        "      name='neuron_per_feature2',\n",
        "      min_value=2,\n",
        "      max_value=200,\n",
        "      step=2,\n",
        "      sampling=None,\n",
        "      default=None,\n",
        "      parent_name=None,\n",
        "      parent_values=None,\n",
        "  )\n",
        "  model.add(tf.keras.layers.GRU(\n",
        "      npf2,\n",
        "      activation = 'tanh',\n",
        "      dropout=dropout_ss,\n",
        "      return_sequences=True,\n",
        "      name='gru2',)\n",
        "      )\n",
        "  \n",
        "  # Third GRU Layer\n",
        "  npf3 = hp.Int(\n",
        "      name='neuron_per_feature3',\n",
        "      min_value=2,\n",
        "      max_value=200,\n",
        "      step=1,\n",
        "      sampling=None,\n",
        "      default=None,\n",
        "      parent_name=None,\n",
        "      parent_values=None,\n",
        "  )\n",
        "  \n",
        "  model.add(tf.keras.layers.GRU(\n",
        "      npf3,\n",
        "      activation = 'tanh',\n",
        "      dropout=dropout_ss,\n",
        "      return_sequences=False,\n",
        "      name='gru3',)\n",
        "      )\n",
        "  \n",
        "  # Final Dense Layer Output\n",
        "  model.add(tf.keras.layers.Dense(1,\n",
        "      activation='sigmoid',\n",
        "      name=\"dense4_output\",)\n",
        "      )\n",
        "    \n",
        "  # Configure Adam optimizer\n",
        "  if optimizer == \"adam\":\n",
        "      optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "  elif optimizer == \"SGD\":\n",
        "      optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
        "  else:\n",
        "      raise(\"Not supported optimizer\")\n",
        "      \n",
        "  model.compile(optimizer=optimizer,\n",
        "                loss=tf.keras.losses.MeanSquaredError(),\n",
        "                metrics=['mean_squared_error'])\n",
        "  \n",
        "  return model\n",
        "\n",
        "# model_builder(kt.HyperParameters())"
      ],
      "metadata": {
        "id": "lMcLHld_Ag0M"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instantiate the tuner and perform hypertuning"
      ],
      "metadata": {
        "id": "YeEn5zX3HKg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_tuner(model, hpo_method, objective, dir_name):\n",
        "  if hpo_method == \"RandomSearch\":\n",
        "      tuner = kt.RandomSearch(model, objective=objective, max_trials=10, executions_per_trial=5,\n",
        "                              project_name=hpo_method, directory=dir_name)\n",
        "  elif hpo_method == \"Hyperband\":\n",
        "      tuner = kt.Hyperband(model, objective=objective, max_epochs=10, executions_per_trial=5,\n",
        "                          project_name=hpo_method)\n",
        "  elif hpo_method == \"BayesianOptimization\":\n",
        "      tuner = kt.BayesianOptimization(model, objective=objective, max_trials=10, executions_per_trial=5,\n",
        "                                      project_name=hpo_method)\n",
        "  return tuner\n",
        "  \n",
        "obj = kt.Objective('mean_squared_error', direction='min')\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='mean_squared_error', patience=5)\n",
        "dir_name = \"v1\"\n",
        "randomsearch_tuner = build_tuner(build_model, \"Hyperband\", obj, dir_name)\n",
        "randomsearch_tuner.search(dataset, epochs=5, callbacks=[stop_early])\n",
        "\n",
        "# # Get the optimal hyperparameters\n",
        "best_hps=randomsearch_tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "best_model = build_model(best_hps)\n",
        "best_model.fit(dataset, epochs=10)"
      ],
      "metadata": {
        "id": "kFUmhrtiHOrb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "outputId": "315150e6-8bfa-4472-81ba-4c0f9c69f088"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 7 Complete [00h 00m 55s]\n",
            "mean_squared_error: 0.004667202290147543\n",
            "\n",
            "Best mean_squared_error So Far: 0.004177630040794611\n",
            "Total elapsed time: 00h 06m 45s\n",
            "\n",
            "Search: Running Trial #8\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "tanh              |relu              |activation\n",
            "adam              |adam              |optimizer\n",
            "0.00010938        |0.0086419         |learning_rate\n",
            "0.1               |0.5               |dropouts_\n",
            "22                |18                |neuron_per_feature1\n",
            "64                |172               |neuron_per_feature2\n",
            "168               |180               |neuron_per_feature3\n",
            "2                 |2                 |tuner/epochs\n",
            "0                 |0                 |tuner/initial_epoch\n",
            "2                 |2                 |tuner/bracket\n",
            "0                 |0                 |tuner/round\n",
            "\n",
            "Epoch 1/2\n",
            "128/128 [==============================] - 8s 35ms/step - loss: 0.0047 - mean_squared_error: 0.0047\n",
            "Epoch 2/2\n",
            "128/128 [==============================] - 3s 20ms/step - loss: 0.0049 - mean_squared_error: 0.0049\n",
            "Epoch 1/2\n",
            "128/128 [==============================] - 8s 29ms/step - loss: 0.0047 - mean_squared_error: 0.0047\n",
            "Epoch 2/2\n",
            "128/128 [==============================] - 3s 17ms/step - loss: 0.0042 - mean_squared_error: 0.0042\n",
            "Epoch 1/2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-570d69b7f534>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mdir_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"v1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mrandomsearch_tuner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_tuner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Hyperband\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mrandomsearch_tuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstop_early\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# # Get the optimal hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras_tuner/engine/base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras_tuner/engine/base_tuner.py\u001b[0m in \u001b[0;36m_try_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrial_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOMPLETED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras_tuner/engine/base_tuner.py\u001b[0m in \u001b[0;36m_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         if self.oracle.get_trial(trial.trial_id).metrics.exists(\n\u001b[1;32m    217\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras_tuner/tuners/hyperband.py\u001b[0m in \u001b[0;36mrun_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0mfit_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"epochs\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tuner/epochs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0mfit_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"initial_epoch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tuner/initial_epoch\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_build_hypermodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras_tuner/engine/tuner.py\u001b[0m in \u001b[0;36mrun_trial\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0mcopied_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"callbacks\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0mobj_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_and_fit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcopied_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0mhistories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras_tuner/engine/tuner.py\u001b[0m in \u001b[0;36m_build_and_fit_model\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \"\"\"\n\u001b[1;32m    209\u001b[0m         \u001b[0mhp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         tuner_utils.validate_trial_results(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras_tuner/engine/hypermodel.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mIf\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    961\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 785\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    786\u001b[0m             *args, **kwds))\n\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2478\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2479\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2480\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2481\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2709\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache_key\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_placeholder_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2711\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2712\u001b[0m           self._function_cache.add(cache_key, cache_key_deletion_observer,\n\u001b[1;32m   2713\u001b[0m                                    graph_function)\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2625\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2626\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 2627\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   2628\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1139\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/auto_control_deps.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, unused_type, unused_value, unused_traceback)\u001b[0m\n\u001b[1;32m    433\u001b[0m           (op_is_stateful(op) and\n\u001b[1;32m    434\u001b[0m            (op.type not in utils.RESOURCE_READ_OPS or\n\u001b[0;32m--> 435\u001b[0;31m             any(output.consumers() for output in op.outputs)))):\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mops_which_must_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/auto_control_deps.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    433\u001b[0m           (op_is_stateful(op) and\n\u001b[1;32m    434\u001b[0m            (op.type not in utils.RESOURCE_READ_OPS or\n\u001b[0;32m--> 435\u001b[0;31m             any(output.consumers() for output in op.outputs)))):\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mops_which_must_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconsumers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    850\u001b[0m         self._as_tf_output())\n\u001b[1;32m    851\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m     return [\n\u001b[0m\u001b[1;32m    853\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_operation_by_name_unsafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconsumer_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    851\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m     return [\n\u001b[0;32m--> 853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_operation_by_name_unsafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    854\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconsumer_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m     ]\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_get_operation_by_name_unsafe\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   4080\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_graph_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4082\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_operation_by_name_unsafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4083\u001b[0m     \"\"\"Returns the `Operation` with the given `name`.\n\u001b[1;32m   4084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(best_hps.values)"
      ],
      "metadata": {
        "id": "Q5ruH_-dVikt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model.save('./keras_prediction_model_v1')"
      ],
      "metadata": {
        "id": "2KDqXx7QrEEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test data using `evaluate`\n",
        "print(\"Evaluate on test data\")\n",
        "results = best_model.evaluate(dataset, batch_size=25)\n",
        "print(\"test loss, test acc:\", results)"
      ],
      "metadata": {
        "id": "bwAAnku2krn5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}