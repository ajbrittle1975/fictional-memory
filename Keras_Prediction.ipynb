{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "iiyCFgOu99A3",
        "clsPvkLM4kVK",
        "0RuhdgYQ4yN6",
        "HOOGKIi4-gTd"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajbrittle1975/fictional-memory/blob/main/Keras_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check Architechture"
      ],
      "metadata": {
        "id": "iiyCFgOu99A3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %reset"
      ],
      "metadata": {
        "id": "bPW0lMyRaK3w"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8V7WxJu-BA7",
        "outputId": "b30c79bf-aa77-4854-976d-5daa27e6bf0e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Feb  9 21:25:36 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   75C    P0    33W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Necessary Libraries"
      ],
      "metadata": {
        "id": "clsPvkLM4kVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://launchpad.net/~mario-mariomedina/+archive/ubuntu/talib/+files'\n",
        "ext = '0.4.0-oneiric1_amd64.deb -qO'\n",
        "!wget $url/libta-lib0_$ext libta.deb\n",
        "!wget $url/ta-lib0-dev_$ext ta.deb\n",
        "!dpkg -i libta.deb ta.deb\n",
        "!pip install ta-lib\n",
        "import talib\n",
        "!pip install numpy pandas matplotlib tensorflow tensorflow-datasets keras keras-tuner alpha-vantage"
      ],
      "metadata": {
        "id": "fqmZ5qGE4sCb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf144627-4d7a-4b3b-fb07-c7d17cce7361"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Reading database ... 128074 files and directories currently installed.)\n",
            "Preparing to unpack libta.deb ...\n",
            "Unpacking libta-lib0 (0.4.0-oneiric1) over (0.4.0-oneiric1) ...\n",
            "Preparing to unpack ta.deb ...\n",
            "Unpacking ta-lib0-dev (0.4.0-oneiric1) over (0.4.0-oneiric1) ...\n",
            "Setting up libta-lib0 (0.4.0-oneiric1) ...\n",
            "Setting up ta-lib0-dev (0.4.0-oneiric1) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ta-lib in /usr/local/lib/python3.8/dist-packages (0.4.25)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from ta-lib) (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.3.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (3.2.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (2.9.2)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.8/dist-packages (4.8.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.8/dist-packages (2.9.0)\n",
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.8/dist-packages (1.2.0)\n",
            "Requirement already satisfied: alpha-vantage in /usr/local/lib/python3.8/dist-packages (2.3.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.51.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (15.0.6.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow) (23.0)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.30.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (4.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: etils[enp,epath]>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (2.25.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (4.64.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (0.1.8)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (0.10.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (2.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (0.3.6)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (1.12.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (5.10.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (5.4.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (7.1.2)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.8/dist-packages (from keras-tuner) (1.0.4)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.8/dist-packages (from keras-tuner) (7.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from alpha-vantage) (3.8.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.8/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets) (3.12.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow-datasets) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow-datasets) (1.24.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.16.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->alpha-vantage) (6.0.4)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->alpha-vantage) (2.1.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->alpha-vantage) (22.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->alpha-vantage) (1.3.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->alpha-vantage) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->alpha-vantage) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->alpha-vantage) (1.8.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (2.0.10)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (2.6.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (0.2.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (0.7.5)\n",
            "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (0.18.2)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (5.7.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-metadata->tensorflow-datasets) (1.58.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.10->ipython->keras-tuner) (0.8.3)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (6.0.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->keras-tuner) (0.2.6)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect->ipython->keras-tuner) (0.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "0RuhdgYQ4yN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "from numpy import array\n",
        "from numpy import hstack\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from pandas import DataFrame as df\n",
        "from pandas import concat\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from keras import layers\n",
        "import keras_tuner as kt\n",
        "from keras.models import Sequential \n",
        "from keras.layers import Activation\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import GRU\n",
        "from keras.utils import timeseries_dataset_from_array\n",
        "\n",
        "from talib import abstract\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "from alpha_vantage.timeseries import TimeSeries\n",
        "from alpha_vantage.techindicators import TechIndicators\n",
        "\n",
        "print(\"All libraries loaded\")"
      ],
      "metadata": {
        "id": "_b0MGjwj41Tx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad01c65b-0432-44e8-9e09-d05b839058a0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All libraries loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stock Configurations"
      ],
      "metadata": {
        "id": "uAu1wLWN9tyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"alpha_vantage\": {\n",
        "        # Claim your free API key here: https://www.alphavantage.co/support/#api-key\n",
        "        \"key\": \"T7GH3GWEIV1DKHB0\",\n",
        "        \"symbol\": \"TQQQ\",\n",
        "        \"outputsize\": \"full\",\n",
        "        \"key_adjusted_close\": \"5. adjusted close\",\n",
        "        \"key_high\": \"2. high\",\n",
        "        \"key_low\": \"3. low\",\n",
        "        \"key_volume\": \"6. volume\",\n",
        "    },\n",
        "    \"data\": {\n",
        "        \"window_size\": 25,\n",
        "        \"train_split_size\": 0.80,\n",
        "        \"dynamic_calcs\": False,\n",
        "    },\n",
        "    \"plots\": {\n",
        "        \"xticks_interval\": 90,  # show a date every 90 days\n",
        "        \"color_actual\": \"#001f3f\",\n",
        "        \"color_train\": \"#3D9970\",\n",
        "        \"color_val\": \"#0074D9\",\n",
        "        \"color_pred_train\": \"#3D9970\",\n",
        "        \"color_pred_val\": \"#0074D9\",\n",
        "        \"color_pred_test\": \"#FF4136\",\n",
        "    },\n",
        "    \"model\": {\n",
        "        \"input_size\": 11,  # we are using 11 features\n",
        "        \"num_lstm_layers\": 2,\n",
        "        \"lstm_size\": 120,\n",
        "        \"dropout\": 0.20,\n",
        "    },\n",
        "    \"training\": {\n",
        "        \"device\": \"cuda\",  # \"cuda\" or \"cpu\"\n",
        "        \"batch_size\": 25,\n",
        "        \"num_epoch\": 200,\n",
        "        \"learning_rate\": 0.01,\n",
        "        \"scheduler_step_size\": 40,\n",
        "    },\n",
        "}\n"
      ],
      "metadata": {
        "id": "F_g-F9jY9nQm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Necessary Functions"
      ],
      "metadata": {
        "id": "cb3ocO-Q-OxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_nan_columns(data_array, data_date):\n",
        "    # return data_array[~np.isnan(data_array).any(axis=1), :]\n",
        "    nan_rows = np.argwhere(np.isnan(data_array).any(axis=1))\n",
        "    print(\"Removing {0} nan rows from data stack...\".format(len(nan_rows)))\n",
        "    data_array = np.delete(data_array, nan_rows, 0)\n",
        "\n",
        "    # given index of elements\n",
        "    # remove largest indices first to not change length\n",
        "    nan_row_list = list(nan_rows.flatten())\n",
        "    data_date = list(data_date)\n",
        "    for ele in sorted(nan_row_list, reverse=True):\n",
        "        data_date.remove(data_date[ele])\n",
        "\n",
        "    return data_array, data_date\n",
        "\n",
        "\n",
        "class Normalizer:\n",
        "    def __init__(self):\n",
        "        self.mu = None\n",
        "        self.sd = None\n",
        "\n",
        "    def fit_transform(self, x):\n",
        "        self.mu = np.nanmean(x, axis=(0), keepdims=True)\n",
        "        self.sd = np.nanstd(x, axis=(0), keepdims=True)\n",
        "        normalized_x = (x - self.mu) / self.sd\n",
        "        return normalized_x\n",
        "\n",
        "    def inverse_transform(self, x):\n",
        "        return (x * self.sd) + self.mu\n",
        "\n",
        "\n",
        "def download_data(config):\n",
        "    ts = TimeSeries(key=config[\"alpha_vantage\"][\"key\"])\n",
        "    ti = TechIndicators(key=config[\"alpha_vantage\"][\"key\"])\n",
        "    dynamic_calcs = config[\"data\"][\"dynamic_calcs\"] ## if we want to use dynamic calculation of indicators this is true\n",
        "\n",
        "    print(\"Obtaining data for symbol: {0}\".format(config[\"alpha_vantage\"][\"symbol\"]))\n",
        "    data, data_meta_data = ts.get_daily_adjusted(\n",
        "        config[\"alpha_vantage\"][\"symbol\"],\n",
        "        outputsize=config[\"alpha_vantage\"][\"outputsize\"],\n",
        "    )\n",
        "\n",
        "    print(\"Sorting data...\")\n",
        "\n",
        "    data_date = [date for date in data.keys()]\n",
        "    data_date.reverse()\n",
        "\n",
        "    print(\"Sorting close price...\")\n",
        "    data_close_price = [\n",
        "        float(data[date][config[\"alpha_vantage\"][\"key_adjusted_close\"]])\n",
        "        for date in data.keys()\n",
        "    ]\n",
        "    data_close_price.reverse()\n",
        "    data_close_price = np.array(data_close_price)\n",
        "    data_price_pct_change = np.diff(data_close_price) / data_close_price[:-1] * 100\n",
        "    data_price_pct_change = np.pad(data_price_pct_change, (1, 0), \"constant\")\n",
        "    data_price_pct_change = data_price_pct_change.reshape(-1, 1)\n",
        "    ## Normalize price percent change\n",
        "    print(\"Normalizing price percent change data...\")\n",
        "    price_pct_change_scaler = MinMaxScaler()\n",
        "    data_price_pct_change_norm = price_pct_change_scaler.fit_transform(\n",
        "        data_price_pct_change\n",
        "    )\n",
        "\n",
        "    print(\"Sorting high prices...\")\n",
        "    data_high_price = [\n",
        "        float(data[date][config[\"alpha_vantage\"][\"key_high\"]]) for date in data.keys()\n",
        "    ]\n",
        "    data_high_price.reverse()\n",
        "    data_high_price = np.array(data_high_price)\n",
        "\n",
        "    print(\"Sorting low prices...\")\n",
        "    data_low_price = [\n",
        "        float(data[date][config[\"alpha_vantage\"][\"key_low\"]]) for date in data.keys()\n",
        "    ]\n",
        "    data_low_price.reverse()\n",
        "    data_low_price = np.array(data_low_price)\n",
        "\n",
        "    print(\"Sorting volume...\")\n",
        "    data_volume = [\n",
        "        float(data[date][config[\"alpha_vantage\"][\"key_volume\"]]) for date in data.keys()\n",
        "    ]\n",
        "    data_volume.reverse()\n",
        "    data_volume = np.array(data_volume)\n",
        "    # Calculate OBV before reshaping data\n",
        "    print(\"Calculating OBV on balance volume...\")\n",
        "    obv_data = abstract.OBV(data_close_price, data_volume)\n",
        "    data_volume = data_volume.reshape(-1, 1)\n",
        "    ## Normalize Volume\n",
        "    print(\"Normalizing Volume Data...\")\n",
        "    volume_scaler = Normalizer()\n",
        "    volume_norm = volume_scaler.fit_transform(data_volume)\n",
        "\n",
        "    print(\"Calculating HT_TRENDLINE...\")\n",
        "    ## Get HT Trend data\n",
        "    ht_trend = abstract.HT_TRENDLINE(data_close_price)\n",
        "    ht_trend = ht_trend.reshape(-1, 1)\n",
        "\n",
        "    ## Normalize HT Trend\n",
        "    print(\"Normalizing HT_TRENDLINE data...\")\n",
        "    ht_trendline_scaler = MinMaxScaler()\n",
        "    ht_trendline_norm = ht_trendline_scaler.fit_transform(ht_trend)\n",
        "    \n",
        "    print(\"Normalizing On Balance Volume data...\")\n",
        "    ## Normalize OBV\n",
        "    obv_norm = obv_data / data_volume\n",
        "    obv_data = obv_data.reshape(-1, 1)\n",
        "    obv_scaler = Normalizer()\n",
        "    obv_norm = obv_scaler.fit_transform(obv_data)  \n",
        "\n",
        "    print(\"Calculating MESA average price...\")\n",
        "    ## Get MESA average price\n",
        "    mama, fama = abstract.MAMA(data_close_price, fastlimit=0.5, slowlimit=0.05)\n",
        "    mesa_hist = mama - fama\n",
        "    mesa_hist = mesa_hist.reshape(-1, 1)\n",
        "    ## Normalize MESA\n",
        "    print(\"Normalizing dynamic MESA trend data...\")\n",
        "    mesa_hist_scaler = Normalizer()\n",
        "    mesa_hist_norm = mesa_hist_scaler.fit_transform(mesa_hist)\n",
        "\n",
        "    print(\"Calculating HT_TREND sine signals...\")\n",
        "    ## Get HT_SINE signals\n",
        "    sine, leadsine = abstract.HT_SINE(data_close_price)\n",
        "    trend = abstract.HT_TRENDMODE(data_close_price)\n",
        "    ht_trend_sine = (sine - leadsine) * trend\n",
        "    ht_trend_sine = ht_trend_sine.reshape(-1, 1)\n",
        "    ## Normalize HT_TREND\n",
        "    print(\"Normalizing dynamic MACD histogram data...\")\n",
        "    ht_trend_sine_scaler = Normalizer()\n",
        "    ht_trend_sine_norm = ht_trend_sine_scaler.fit_transform(ht_trend_sine)\n",
        "\n",
        "    print(\"Calculating HT_PERIODS for dynamic indicators...\")\n",
        "    # Get period of data close price\n",
        "    ht_period = abstract.HT_DCPERIOD(data_close_price)\n",
        "    ht_period = np.around(ht_period)  # the instant period is optimal for oscillators\n",
        "\n",
        "    print(\"Provisioning arrays for dynamic indicators...\")\n",
        "    # Provision arrays for additional indicators\n",
        "    num_data_points = len(data_date)\n",
        "    adx_ht = np.zeros(num_data_points, dtype=float)\n",
        "    aroon_osc_ht = np.zeros(num_data_points, dtype=float)\n",
        "    rsi_ht = np.zeros(num_data_points, dtype=float)\n",
        "    macd_ht = np.zeros(num_data_points, dtype=float)\n",
        "    macdsignal_ht = np.zeros(num_data_points, dtype=float)\n",
        "    macdhist_ht = np.zeros(num_data_points, dtype=float)\n",
        "    slowk_ht = np.zeros(num_data_points, dtype=float)\n",
        "    slowd_ht = np.zeros(num_data_points, dtype=float)\n",
        "    bb_lowerband_ht = np.zeros(num_data_points, dtype=float)\n",
        "    bb_upperband_ht = np.zeros(num_data_points, dtype=float)\n",
        "    bb_pct_ht = np.zeros(num_data_points, dtype=float)\n",
        "    ult_osc_ht = np.zeros(num_data_points, dtype=float)\n",
        "\n",
        "    # Get frequency-adjusted ADX\n",
        "    print(\"Calculating dynamic ADX...\")\n",
        "    if dynamic_calcs:\n",
        "      i = 0\n",
        "      for period in ht_period:\n",
        "          if not math.isnan(period):\n",
        "              period = int(period)\n",
        "              slowperiod = (\n",
        "                  period if period % 2 == 0 else period + 1\n",
        "              )  # Ensure slow period is even\n",
        "              adx = abstract.ADX(\n",
        "                  data_high_price, data_low_price, data_close_price, timeperiod=slowperiod\n",
        "              )\n",
        "              adx_ht[i] = adx[i]\n",
        "\n",
        "          i += 1\n",
        "      else:\n",
        "        adx_ht = abstract.ADX(\n",
        "          data_high_price, data_low_price, data_close_price, timeperiod=14\n",
        "          )\n",
        "        \n",
        "    adx_ht = adx_ht.reshape(-1, 1)\n",
        "    ## Normalize ADX\n",
        "    print(\"Normalizing dynamic ADX data...\")\n",
        "    adx_norm = adx_ht / 100\n",
        "\n",
        "    # Get frequency-adjusted Aroon Oscillator\n",
        "    print(\"Calculating dynamic AROONOSC...\")\n",
        "    if dynamic_calcs:\n",
        "      i = 0\n",
        "      for period in ht_period:\n",
        "          if not math.isnan(period):\n",
        "              period = int(period)\n",
        "              slowperiod = (\n",
        "                  period if period % 2 == 0 else period + 1\n",
        "              )  # Ensure slow period is even\n",
        "              aroon_osc = abstract.AROONOSC(\n",
        "                  data_high_price, data_low_price, timeperiod=int(slowperiod/2)\n",
        "              )\n",
        "              aroon_osc_ht[i] = aroon_osc[i]\n",
        "\n",
        "          i += 1\n",
        "      else:\n",
        "        aroon_osc_ht = abstract.AROONOSC(\n",
        "            data_high_price, data_low_price, timeperiod=14\n",
        "        )\n",
        "\n",
        "    aroon_osc_ht = aroon_osc_ht.reshape(-1, 1)\n",
        "    ## Normalize Aroon Oscillator\n",
        "    print(\"Normalizing dynamic AROONOSC data...\")\n",
        "    aroon_osc_norm = (aroon_osc_ht + 100) / 200  # oscillates between +/- 100\n",
        "\n",
        "    # Get frequency-adjusted MACD\n",
        "    print(\"Calculating dynamic MACD...\")\n",
        "    if dynamic_calcs:\n",
        "      i = 0\n",
        "      for period in ht_period:\n",
        "          if not math.isnan(period):\n",
        "              period = int(period)\n",
        "              slowperiod = (\n",
        "                  period if period % 2 == 0 else period + 1\n",
        "              )  # Ensure slow period is even\n",
        "              fastperiod = int((slowperiod / 2) - 1)\n",
        "              signalperiod = math.floor(0.75 * fastperiod)\n",
        "              macd, macdsignal, macdhist = abstract.MACD(\n",
        "                  data_close_price,\n",
        "                  fastperiod=fastperiod,\n",
        "                  slowperiod=slowperiod,\n",
        "                  signalperiod=signalperiod,\n",
        "              )\n",
        "              macd_ht[i] = macd[i]\n",
        "              macdsignal_ht[i] = macdsignal[i]\n",
        "              macdhist_ht[i] = macdhist[i]\n",
        "\n",
        "          i += 1\n",
        "\n",
        "      else:\n",
        "        macd_ht, macdsignal_ht, macdhist_ht = abstract.MACD(\n",
        "            data_close_price, fastperiod=12, slowperiod=26, signalperiod=9\n",
        "        )\n",
        "\n",
        "\n",
        "    macdhist_ht = macdhist_ht.reshape(-1, 1)\n",
        "    ## Normalize MACD\n",
        "    print(\"Normalizing dynamic MACD histogram data...\")\n",
        "    macd_scaler = Normalizer()\n",
        "    macd_norm = macd_scaler.fit_transform(macdhist_ht)\n",
        "\n",
        "    # Get frequency-adjusted Bollinger Bands\n",
        "    print(\"Calculating dynamic BBANDS...\")\n",
        "    if dynamic_calcs:\n",
        "      i = 0\n",
        "      for period in ht_period:\n",
        "          if not math.isnan(period):\n",
        "              period = int(period)\n",
        "              slowperiod = (\n",
        "                  period if period % 2 == 0 else period + 1\n",
        "              )  # Ensure slow period is even\n",
        "              bb_upperband, bb_middleband, bb_lowerband = abstract.BBANDS(\n",
        "                  data_close_price,\n",
        "                  timeperiod=slowperiod,\n",
        "                  nbdevup=float(2),\n",
        "                  nbdevdn=float(2),\n",
        "                  matype=1,\n",
        "              )\n",
        "              bb_upperband_ht[i] = bb_upperband[i]\n",
        "              bb_lowerband_ht[i] = bb_lowerband[i]\n",
        "              bb_pct_ht[i] = (data_close_price[i] - bb_lowerband[i]) / (\n",
        "                  bb_upperband[i] - bb_lowerband[i]\n",
        "              )\n",
        "\n",
        "          i += 1\n",
        "\n",
        "      else:\n",
        "        bb_upperband, bb_middleband, bb_lowerband = abstract.BBANDS(\n",
        "            data_close_price, timeperiod=5, nbdevup=2, nbdevdn=2, matype=0\n",
        "        )\n",
        "        bb_pct_ht = (data_close_price - bb_lowerband) / (\n",
        "            bb_upperband - bb_lowerband\n",
        "        )\n",
        "\n",
        "    bb_pct_ht = bb_pct_ht.reshape(-1, 1)\n",
        "    ## Normalize MACD\n",
        "    print(\"Normalizing dynamic BBANDS Percentage data...\")\n",
        "    bb_pct_scaler = Normalizer()\n",
        "    bb_pct_norm = bb_pct_scaler.fit_transform(bb_pct_ht)\n",
        "\n",
        "    # Get frequency-adjusted RSI\n",
        "    print(\"Calculating dynamic RSI...\")\n",
        "    if dynamic_calcs:\n",
        "      i = 0\n",
        "      for period in ht_period:\n",
        "          if not math.isnan(period):\n",
        "              period = (\n",
        "                  period / 2 if period % 2 == 0 else (period + 1) / 2\n",
        "              )  # Ensure slow period is even and divide by 2\n",
        "              period = int(period)\n",
        "              rsi = abstract.RSI(\n",
        "                  data_close_price,\n",
        "                  timeperiod=period,\n",
        "              )\n",
        "              rsi_ht[i] = rsi[i]\n",
        "\n",
        "          i += 1\n",
        "      else:\n",
        "          rsi_ht = abstract.RSI(\n",
        "                  data_close_price, timeperiod=14\n",
        "              )\n",
        "        \n",
        "\n",
        "    rsi_ht = rsi_ht.reshape(-1, 1)\n",
        "    ## Normalize RSI\n",
        "    print(\"Normalizing dynamic RSI data...\")\n",
        "    rsi_norm = rsi_ht / 100\n",
        "\n",
        "    # Get frequency-adjusted Stochastic\n",
        "    print(\"Calculating dynamic Stochastic...\")\n",
        "    if dynamic_calcs:\n",
        "      i = 0\n",
        "      for period in ht_period:\n",
        "          if not math.isnan(period):\n",
        "              period = (\n",
        "                  period / 2 if period % 2 == 0 else (period + 1) / 2\n",
        "              )  # Ensure slow period is even and divide by 2\n",
        "              period = int(period)\n",
        "              slowk, slowd = abstract.STOCH(\n",
        "                  data_high_price,\n",
        "                  data_low_price,\n",
        "                  data_close_price,\n",
        "                  fastk_period=period,\n",
        "                  slowk_period=3,\n",
        "                  slowk_matype=1,\n",
        "                  slowd_period=3,\n",
        "                  slowd_matype=1,\n",
        "              )\n",
        "              slowk_ht[i] = slowk[i]\n",
        "              slowd_ht[i] = slowd[i]\n",
        "\n",
        "          i += 1\n",
        "      else:\n",
        "        slowk_ht, slowd_ht = abstract.STOCH(\n",
        "          data_high_price,\n",
        "          data_low_price,\n",
        "          data_close_price,\n",
        "          fastk_period=5, \n",
        "          slowk_period=3, \n",
        "          slowk_matype=0, \n",
        "          slowd_period=3, \n",
        "          slowd_matype=0\n",
        "        )\n",
        "\n",
        "    stochastic_hist_ht = slowk_ht - slowd_ht\n",
        "    stochastic_hist_ht = stochastic_hist_ht.reshape(-1, 1)\n",
        "    ## Normalize dynamic stochastic histogram\n",
        "    print(\"Normalizing dynamic stochastic histogram data...\")\n",
        "    stochastic_hist_scaler = Normalizer()\n",
        "    stochastic_hist_norm = stochastic_hist_scaler.fit_transform(stochastic_hist_ht)\n",
        "    \n",
        "    # Get frequency-adjusted Ultimate Oscillator\n",
        "    print(\"Calculating dynamic ULTOSC...\")\n",
        "    if dynamic_calcs:\n",
        "      i = 0\n",
        "      for period in ht_period:\n",
        "          if not math.isnan(period):\n",
        "              period = (\n",
        "                  period / 2 if period % 2 == 0 else (period + 1) / 2\n",
        "              )  # Ensure slow period is even and divide by 2\n",
        "              period = int(period)\n",
        "              ult_osc = abstract.ULTOSC(\n",
        "                  data_high_price,\n",
        "                  data_low_price,\n",
        "                  data_close_price,\n",
        "                  timeperiod1=period,\n",
        "                  timeperiod2=period*2,\n",
        "                  timeperiod3=period*4,\n",
        "              )\n",
        "              ult_osc_ht[i] = ult_osc[i]\n",
        "\n",
        "          i += 1\n",
        "      else:\n",
        "        ult_osc_ht = abstract.ULTOSC(\n",
        "            data_high_price,\n",
        "            data_low_price,\n",
        "            data_close_price,\n",
        "            timeperiod1=7, \n",
        "            timeperiod2=14, \n",
        "            timeperiod3=28\n",
        "        )\n",
        "\n",
        "    ult_osc_ht = ult_osc_ht.reshape(-1, 1)\n",
        "    ## Normalize dynamic stochastic histogram\n",
        "    print(\"Normalizing dynamic ULTOSC data...\")\n",
        "    \n",
        "\n",
        "    # print(\"volume_norm: {0}\".format(volume_norm.shape))\n",
        "    # print(\"obv_norm: {0}\".format(obv_norm.shape))\n",
        "    # print(\"mesa_hist_norm: {0}\".format(mesa_hist_norm.shape))\n",
        "    # print(\"ht_trend_sine: {0}\".format(ht_trend_sine.shape))\n",
        "    # print(\"macd_norm: {0}\".format(volume_norm.shape))\n",
        "    # print(\"adx_norm: {0}\".format(adx_norm.shape))\n",
        "    # print(\"aroon_osc_norm: {0}\".format(aroon_osc_norm.shape))\n",
        "    # print(\"bb_pct_norm: {0}\".format(bb_pct_norm.shape))\n",
        "    # print(\"rsi_norm: {0}\".format(rsi_norm.shape))\n",
        "    # print(\"ult_osc_ht: {0}\".format(ult_osc_ht.shape))\n",
        "    # print(\"data_price_pct_change_norm: {0}\".format(data_price_pct_change_norm.shape))\n",
        "\n",
        "\n",
        "\n",
        "    data_stack = hstack(\n",
        "        (\n",
        "            volume_norm,\n",
        "            obv_norm,\n",
        "            mesa_hist_norm,\n",
        "            ht_trend_sine,\n",
        "            macd_norm,\n",
        "            adx_norm,\n",
        "            aroon_osc_norm,\n",
        "            bb_pct_norm,\n",
        "            rsi_norm,\n",
        "            stochastic_hist_norm,\n",
        "            ult_osc_ht,\n",
        "            data_price_pct_change_norm,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    print(data_stack)\n",
        "    # print(data_stack.size)\n",
        "    data_stack, data_date = remove_nan_columns(data_stack, data_date)\n",
        "    num_data_points = data_stack.shape[0]\n",
        "    display_date_range = (\n",
        "        \"from \" + data_date[0] + \" to \" + data_date[num_data_points - 1]\n",
        "    )\n",
        "    print(\"Number data points:\", num_data_points, display_date_range)\n",
        "\n",
        "    # print(data_stack)\n",
        "    # print(data_stack.size)\n",
        "    print(DAD)\n",
        "\n",
        "    return data_date, data_stack, num_data_points, display_date_range, price_pct_change_scaler"
      ],
      "metadata": {
        "id": "rZs_-wGu-SmC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Stock Data"
      ],
      "metadata": {
        "id": "HOOGKIi4-gTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_date, data_stack, num_data_points, display_date_range, price_pct_change_scaler = download_data(config)\n"
      ],
      "metadata": {
        "id": "tG_3OUkj-iq5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cb5c6636-3d82-4185-efbe-0b18efef7ec3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining data for symbol: TQQQ\n",
            "Sorting data...\n",
            "Sorting close price...\n",
            "Normalizing price percent change data...\n",
            "Sorting high prices...\n",
            "Sorting low prices...\n",
            "Sorting volume...\n",
            "Calculating OBV on balance volume...\n",
            "Normalizing Volume Data...\n",
            "Calculating HT_TRENDLINE...\n",
            "Normalizing HT_TRENDLINE data...\n",
            "Normalizing On Balance Volume data...\n",
            "Calculating MESA average price...\n",
            "Normalizing dynamic MESA trend data...\n",
            "Calculating HT_TREND sine signals...\n",
            "Normalizing dynamic MACD histogram data...\n",
            "Calculating HT_PERIODS for dynamic indicators...\n",
            "Provisioning arrays for dynamic indicators...\n",
            "Calculating dynamic ADX...\n",
            "Normalizing dynamic ADX data...\n",
            "Calculating dynamic AROONOSC...\n",
            "Normalizing dynamic AROONOSC data...\n",
            "Calculating dynamic MACD...\n",
            "Normalizing dynamic MACD histogram data...\n",
            "Calculating dynamic BBANDS...\n",
            "Normalizing dynamic BBANDS Percentage data...\n",
            "Calculating dynamic RSI...\n",
            "Normalizing dynamic RSI data...\n",
            "Calculating dynamic Stochastic...\n",
            "Normalizing dynamic stochastic histogram data...\n",
            "Calculating dynamic ULTOSC...\n",
            "Normalizing dynamic ULTOSC data...\n",
            "[[-0.51368815 -0.50175314         nan ...         nan  0.\n",
            "   0.5608328 ]\n",
            " [-0.5050234  -0.50068806         nan ...         nan  0.\n",
            "   0.56749453]\n",
            " [-0.50330985 -0.49949698         nan ...         nan  0.\n",
            "   0.62386205]\n",
            " ...\n",
            " [ 2.75030978 -1.99372534  2.03061853 ...         nan  0.\n",
            "   0.5193865 ]\n",
            " [ 4.73217924 -1.60755242  1.92681777 ...         nan  0.\n",
            "   0.66159508]\n",
            " [ 3.1431535  -1.87687932  1.55707881 ...         nan  0.\n",
            "   0.47333987]]\n",
            "Removing 3271 nan rows from data stack...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-c98ea9bb457d>:25: RuntimeWarning: invalid value encountered in true_divide\n",
            "  normalized_x = (x - self.mu) / self.sd\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-808641e109c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_stack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_data_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_date_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprice_pct_change_scaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-c98ea9bb457d>\u001b[0m in \u001b[0;36mdownload_data\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Obtaining data for symbol: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"alpha_vantage\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"symbol\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     data, data_meta_data = ts.get_daily_adjusted(\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"alpha_vantage\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"symbol\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0moutputsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"alpha_vantage\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"outputsize\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split Data Into Train and Test Groups"
      ],
      "metadata": {
        "id": "JLcfFoXtH__R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters from config\n",
        "batch_size = config[\"training\"][\"batch_size\"]\n",
        "epochs = config[\"training\"][\"num_epoch\"]\n",
        "dropout=config[\"model\"][\"dropout\"]\n",
        "window_size = config[\"data\"][\"window_size\"]\n",
        "features = config[\"model\"][\"input_size\"]\n",
        "neuron_per_feature = 8\n",
        "\n",
        "# split into train and test sets\n",
        "input_data = data_stack[:, :-1] # all but last column\n",
        "target_data = data_stack[:, -1] # for last column\n",
        "print(target_data)\n",
        "target_data = np.pad(target_data,((1,0)), mode='constant')[:-1] # Shift so we are getting target for next day\n",
        "target_data = np.reshape(target_data, (input_data.shape[0], 1) )\n",
        "print(target_data)\n",
        "\n",
        "dataset = keras.utils.timeseries_dataset_from_array(\n",
        "    input_data, \n",
        "    target_data, \n",
        "    sequence_length=window_size, \n",
        "    batch_size=batch_size,\n",
        "    seed=None)\n",
        "\n",
        "batch_count = int(input_data.shape[0]/batch_size)"
      ],
      "metadata": {
        "id": "NL9ozoDwFsbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### GRU Network"
      ],
      "metadata": {
        "id": "ES-fPK5tAeYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(hp):\n",
        "  dnn_units_min, dnn_units_max = 32, 512\n",
        "  active_func_ss = ['relu', 'tanh']\n",
        "  optimizer_ss = ['adam']\n",
        "  lr_min, lr_max = 1e-4, 1e-1\n",
        "  \n",
        "  active_func = hp.Choice('activation', active_func_ss)\n",
        "  optimizer = hp.Choice('optimizer', optimizer_ss)\n",
        "  lr = hp.Float('learning_rate', min_value=lr_min, max_value=lr_max, sampling='log')\n",
        "  dropout_ss = hp.Choice(\"dropouts_\", [0.05, 0.1, 0.15, 0.2])\n",
        "    \n",
        "  # Build Model\n",
        "  # Sequential\n",
        "  model = tf.keras.Sequential()\n",
        "\n",
        "  # First GRU Layer\n",
        "  # input shape should be form (batch, window_size, features)\n",
        "  npf1 = hp.Int(\n",
        "      name='neuron_per_feature1',\n",
        "      min_value=10,\n",
        "      max_value=30,\n",
        "      step=2,\n",
        "      sampling=None,\n",
        "      default=None,\n",
        "      parent_name=None,\n",
        "      parent_values=None,\n",
        "  )\n",
        "  model.add(tf.keras.layers.GRU(\n",
        "      npf1, \n",
        "      input_shape=(window_size, features),\n",
        "      activation = 'tanh',\n",
        "      dropout=dropout_ss,\n",
        "      return_sequences=True,\n",
        "      name='gru1',)\n",
        "      )\n",
        "  \n",
        "  # Second GRU Layer\n",
        "  npf2 = hp.Int(\n",
        "      name='neuron_per_feature2',\n",
        "      min_value=18,\n",
        "      max_value=58,\n",
        "      step=2,\n",
        "      sampling=None,\n",
        "      default=None,\n",
        "      parent_name=None,\n",
        "      parent_values=None,\n",
        "  )\n",
        "  model.add(tf.keras.layers.GRU(\n",
        "      npf2,\n",
        "      activation = 'tanh',\n",
        "      dropout=dropout_ss,\n",
        "      return_sequences=True,\n",
        "      name='gru2',)\n",
        "      )\n",
        "  \n",
        "  # Third GRU Layer\n",
        "  npf3 = hp.Int(\n",
        "      name='neuron_per_feature3',\n",
        "      min_value=38,\n",
        "      max_value=68,\n",
        "      step=1,\n",
        "      sampling=None,\n",
        "      default=None,\n",
        "      parent_name=None,\n",
        "      parent_values=None,\n",
        "  )\n",
        "  \n",
        "  model.add(tf.keras.layers.GRU(\n",
        "      npf3,\n",
        "      activation = 'tanh',\n",
        "      dropout=dropout_ss,\n",
        "      return_sequences=False,\n",
        "      name='gru3',)\n",
        "      )\n",
        "  \n",
        "  # Final Dense Layer Output\n",
        "  model.add(tf.keras.layers.Dense(1,\n",
        "      activation='sigmoid',\n",
        "      name=\"dense4_output\",)\n",
        "      )\n",
        "    \n",
        "  # Configure Adam optimizer\n",
        "  if optimizer == \"adam\":\n",
        "      optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "  elif optimizer == \"SGD\":\n",
        "      optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
        "  else:\n",
        "      raise(\"Not supported optimizer\")\n",
        "      \n",
        "  model.compile(optimizer=optimizer,\n",
        "                loss=tf.keras.losses.MeanSquaredError(),\n",
        "                metrics=['mean_squared_error'])\n",
        "  \n",
        "  return model\n",
        "\n",
        "# model_builder(kt.HyperParameters())"
      ],
      "metadata": {
        "id": "lMcLHld_Ag0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instantiate the tuner and perform hypertuning"
      ],
      "metadata": {
        "id": "YeEn5zX3HKg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_tuner(model, hpo_method, objective, dir_name):\n",
        "  if hpo_method == \"RandomSearch\":\n",
        "      tuner = kt.RandomSearch(model, objective=objective, max_trials=4, executions_per_trial=5,\n",
        "                              project_name=hpo_method, directory=dir_name)\n",
        "  elif hpo_method == \"Hyperband\":\n",
        "      tuner = kt.Hyperband(model, objective=objective, max_epochs=4, executions_per_trial=5,\n",
        "                          project_name=hpo_method)\n",
        "  elif hpo_method == \"BayesianOptimization\":\n",
        "      tuner = kt.BayesianOptimization(model, objective=objective, max_trials=4, executions_per_trial=5,\n",
        "                                      project_name=hpo_method)\n",
        "  return tuner\n",
        "  \n",
        "obj = kt.Objective('mean_squared_error', direction='min')\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='mean_squared_error', patience=4)\n",
        "dir_name = \"v2\"\n",
        "randomsearch_tuner = build_tuner(build_model, \"Hyperband\", obj, dir_name)\n",
        "randomsearch_tuner.search(dataset, epochs=5, callbacks=[stop_early])\n",
        "\n",
        "# # Get the optimal hyperparameters\n",
        "best_hps=randomsearch_tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "best_model = build_model(best_hps)\n",
        "best_model.fit(dataset, epochs=10)"
      ],
      "metadata": {
        "id": "kFUmhrtiHOrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(best_hps.values)"
      ],
      "metadata": {
        "id": "Q5ruH_-dVikt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model.save('./keras_prediction_model_v1')"
      ],
      "metadata": {
        "id": "2KDqXx7QrEEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test data using `evaluate`\n",
        "print(\"Evaluate on test data\")\n",
        "results = best_model.evaluate(dataset, batch_size=25)\n",
        "print(\"test loss, test acc:\", results)"
      ],
      "metadata": {
        "id": "bwAAnku2krn5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}