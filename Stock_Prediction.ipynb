{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7aWQQ7cxoT/L47E5m9PIw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajbrittle1975/fictional-memory/blob/main/Stock_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Necessary Libraries\n",
        "**TA-Lib Libraries**"
      ],
      "metadata": {
        "id": "FqThLi-Lcucv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://launchpad.net/~mario-mariomedina/+archive/ubuntu/talib/+files'\n",
        "ext = '0.4.0-oneiric1_amd64.deb -qO'\n",
        "!wget $url/libta-lib0_$ext libta.deb\n",
        "!wget $url/ta-lib0-dev_$ext ta.deb\n",
        "!dpkg -i libta.deb ta.deb\n",
        "!pip install ta-lib\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIl9JM8rcww5",
        "outputId": "b2fbe114-9aa1-4d00-cd3d-4079febc7bf0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting previously unselected package libta-lib0.\n",
            "(Reading database ... 124016 files and directories currently installed.)\n",
            "Preparing to unpack libta.deb ...\n",
            "Unpacking libta-lib0 (0.4.0-oneiric1) ...\n",
            "Selecting previously unselected package ta-lib0-dev.\n",
            "Preparing to unpack ta.deb ...\n",
            "Unpacking ta-lib0-dev (0.4.0-oneiric1) ...\n",
            "Setting up libta-lib0 (0.4.0-oneiric1) ...\n",
            "Setting up ta-lib0-dev (0.4.0-oneiric1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ta-lib\n",
            "  Downloading TA-Lib-0.4.25.tar.gz (271 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.0/272.0 KB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from ta-lib) (1.21.6)\n",
            "Building wheels for collected packages: ta-lib\n",
            "  Building wheel for ta-lib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta-lib: filename=TA_Lib-0.4.25-cp38-cp38-linux_x86_64.whl size=1820823 sha256=6e89b2d24b8e14a7dab50ef6fc8dab0eef8f7abcfbdb4baf29e3a810cf006b12\n",
            "  Stored in directory: /root/.cache/pip/wheels/da/72/bf/464831127ee8d6d9a5b76340a6a2f115182e159309dc3067ca\n",
            "Successfully built ta-lib\n",
            "Installing collected packages: ta-lib\n",
            "Successfully installed ta-lib-0.4.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Alphavantage Libraries**"
      ],
      "metadata": {
        "id": "8QknYHV_djKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install alpha-vantage"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwI9kxgtdmrc",
        "outputId": "7ecd347b-9ae8-494c-8513-5a91b3fae6de"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting alpha-vantage\n",
            "  Downloading alpha_vantage-2.3.1-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from alpha-vantage) (2.25.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from alpha-vantage) (3.8.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->alpha-vantage) (22.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->alpha-vantage) (1.3.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->alpha-vantage) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->alpha-vantage) (2.1.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->alpha-vantage) (1.8.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->alpha-vantage) (6.0.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->alpha-vantage) (1.3.1)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->alpha-vantage) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->alpha-vantage) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->alpha-vantage) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->alpha-vantage) (2.10)\n",
            "Installing collected packages: alpha-vantage\n",
            "Successfully installed alpha-vantage-2.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pytorch Libraries**"
      ],
      "metadata": {
        "id": "z9j7GqQwe28Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQiWawYde6-3",
        "outputId": "ac7fbb43-33b3-4d25-a73a-ff646f239294"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.0+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.14.0+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.25.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run Imports"
      ],
      "metadata": {
        "id": "8ZRWxANq0zk9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbEVskiBcXC5",
        "outputId": "10ec6e5c-b334-40fb-cf58-bcc02a0ff7d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All libraries loaded\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from talib import abstract\n",
        "import math\n",
        "\n",
        "# multivariate data preparation\n",
        "from numpy import array\n",
        "from numpy import hstack\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn import datasets\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "from alpha_vantage.timeseries import TimeSeries\n",
        "from alpha_vantage.techindicators import TechIndicators\n",
        "\n",
        "print(\"All libraries loaded\")\n",
        "plot_on = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Stock Configuration"
      ],
      "metadata": {
        "id": "TBw9P_--fNEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"alpha_vantage\": {\n",
        "        # Claim your free API key here: https://www.alphavantage.co/support/#api-key\n",
        "        \"key\": \"T7GH3GWEIV1DKHB0\",\n",
        "        \"symbol\": \"TQQQ\",\n",
        "        \"outputsize\": \"full\",\n",
        "        \"key_adjusted_close\": \"5. adjusted close\",\n",
        "        \"key_high\": \"2. high\",\n",
        "        \"key_low\": \"3. low\",\n",
        "    },\n",
        "    \"data\": {\n",
        "        \"window_size\": 10,\n",
        "        \"train_split_size\": 0.80,\n",
        "    },\n",
        "    \"plots\": {\n",
        "        \"xticks_interval\": 90,  # show a date every 90 days\n",
        "        \"color_actual\": \"#001f3f\",\n",
        "        \"color_train\": \"#3D9970\",\n",
        "        \"color_val\": \"#0074D9\",\n",
        "        \"color_pred_train\": \"#3D9970\",\n",
        "        \"color_pred_val\": \"#0074D9\",\n",
        "        \"color_pred_test\": \"#FF4136\",\n",
        "    },\n",
        "    \"model\": {\n",
        "        \"input_size\": 8,  # since we are only using 1 feature, close price\n",
        "        \"num_lstm_layers\": 2,\n",
        "        \"lstm_size\": 120,\n",
        "        \"dropout\": 0.20,\n",
        "    },\n",
        "    \"training\": {\n",
        "        \"device\": \"cuda\",  # \"cuda\" or \"cpu\"\n",
        "        \"batch_size\": 64,\n",
        "        \"num_epoch\": 300,\n",
        "        \"learning_rate\": 0.01,\n",
        "        \"scheduler_step_size\": 40,\n",
        "    },\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "luivZkFMfUQ9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Stock Data & Run Indicators "
      ],
      "metadata": {
        "id": "UhtOqPwe1FlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_nan_columns(data_array, data_date):\n",
        "    # return data_array[~np.isnan(data_array).any(axis=1), :]\n",
        "    nan_rows = np.argwhere(np.isnan(data_array).any(axis=1))\n",
        "    print(\"Removing {0} nan rows from data stack...\".format(len(nan_rows)))\n",
        "    data_array = np.delete(data_array, nan_rows, 0)\n",
        "\n",
        "    # given index of elements\n",
        "    # remove largest indices first to not change length\n",
        "    nan_row_list = list(nan_rows.flatten())\n",
        "    data_date = list(data_date)\n",
        "    for ele in sorted(nan_row_list, reverse=True):\n",
        "        data_date.remove(data_date[ele])\n",
        "\n",
        "    return data_array, data_date\n",
        "\n",
        "\n",
        "class Normalizer:\n",
        "    def __init__(self):\n",
        "        self.mu = None\n",
        "        self.sd = None\n",
        "\n",
        "    def fit_transform(self, x):\n",
        "        self.mu = np.nanmean(x, axis=(0), keepdims=True)\n",
        "        self.sd = np.nanstd(x, axis=(0), keepdims=True)\n",
        "        normalized_x = (x - self.mu) / self.sd\n",
        "        return normalized_x\n",
        "\n",
        "    def inverse_transform(self, x):\n",
        "        return (x * self.sd) + self.mu\n",
        "\n",
        "\n",
        "def download_data(config):\n",
        "    ts = TimeSeries(key=config[\"alpha_vantage\"][\"key\"])\n",
        "    ti = TechIndicators(key=config[\"alpha_vantage\"][\"key\"])\n",
        "\n",
        "    print(\"Obtaining data for symbol: {0}\".format(config[\"alpha_vantage\"][\"symbol\"]))\n",
        "    data, data_meta_data = ts.get_daily_adjusted(\n",
        "        config[\"alpha_vantage\"][\"symbol\"],\n",
        "        outputsize=config[\"alpha_vantage\"][\"outputsize\"],\n",
        "    )\n",
        "\n",
        "    print(\"Sorting data...\")\n",
        "\n",
        "    data_date = [date for date in data.keys()]\n",
        "    data_date.reverse()\n",
        "\n",
        "    print(\"Sorting close price...\")\n",
        "    data_close_price = [\n",
        "        float(data[date][config[\"alpha_vantage\"][\"key_adjusted_close\"]])\n",
        "        for date in data.keys()\n",
        "    ]\n",
        "    data_close_price.reverse()\n",
        "    data_close_price = np.array(data_close_price)\n",
        "    data_price_pct_change = np.diff(data_close_price) / data_close_price[:-1] * 100\n",
        "    data_price_pct_change = np.pad(data_price_pct_change, (1, 0), \"constant\")\n",
        "    data_price_pct_change = data_price_pct_change.reshape(-1, 1)\n",
        "    ## Normalize price percent change\n",
        "    print(\"Normalizing price percent change data...\")\n",
        "    price_pct_change_scaler = MinMaxScaler()\n",
        "    data_price_pct_change_norm = price_pct_change_scaler.fit_transform(\n",
        "        data_price_pct_change\n",
        "    )\n",
        "\n",
        "    print(\"Sorting high prices...\")\n",
        "    data_high_price = [\n",
        "        float(data[date][config[\"alpha_vantage\"][\"key_high\"]]) for date in data.keys()\n",
        "    ]\n",
        "    data_high_price.reverse()\n",
        "    data_high_price = np.array(data_high_price)\n",
        "\n",
        "    print(\"Sorting low prices...\")\n",
        "    data_low_price = [\n",
        "        float(data[date][config[\"alpha_vantage\"][\"key_low\"]]) for date in data.keys()\n",
        "    ]\n",
        "    data_low_price.reverse()\n",
        "    data_low_price = np.array(data_high_price)\n",
        "\n",
        "    print(\"Calculating HT_TRENDLINE...\")\n",
        "    ## Get HT Trend data\n",
        "    ht_trend = abstract.HT_TRENDLINE(data_close_price)\n",
        "    ht_trend = ht_trend.reshape(-1, 1)\n",
        "    ## Normalize MESA\n",
        "    print(\"Normalizing HT_TRENDLINE data...\")\n",
        "    ht_trendline_scaler = MinMaxScaler()\n",
        "    ht_trendline_norm = ht_trendline_scaler.fit_transform(ht_trend)\n",
        "\n",
        "    print(\"Calculating MESA average price...\")\n",
        "    ## Get MESA average price\n",
        "    mama, fama = abstract.MAMA(data_close_price, fastlimit=0.5, slowlimit=0.05)\n",
        "    mesa_hist = mama - fama\n",
        "    mesa_hist = mesa_hist.reshape(-1, 1)\n",
        "    ## Normalize MESA\n",
        "    print(\"Normalizing dynamic MACD histogram data...\")\n",
        "    mesa_hist_scaler = Normalizer()\n",
        "    mesa_hist_norm = mesa_hist_scaler.fit_transform(mesa_hist)\n",
        "\n",
        "    print(\"Calculating HT_TREND sine signals...\")\n",
        "    ## Get HT_SINE signals\n",
        "    sine, leadsine = abstract.HT_SINE(data_close_price)\n",
        "    trend = abstract.HT_TRENDMODE(data_close_price)\n",
        "    ht_trend_sine = (sine - leadsine) * trend\n",
        "    ht_trend_sine = ht_trend_sine.reshape(-1, 1)\n",
        "    ## Normalize HT_TREND\n",
        "    print(\"Normalizing dynamic MACD histogram data...\")\n",
        "    ht_trend_sine_scaler = Normalizer()\n",
        "    ht_trend_sine_norm = ht_trend_sine_scaler.fit_transform(ht_trend_sine)\n",
        "\n",
        "    print(\"Calculating HT_PERIODS for dynamic indicators...\")\n",
        "    # Get period of data close price\n",
        "    ht_period = abstract.HT_DCPERIOD(data_close_price)\n",
        "    ht_period = np.around(ht_period)  # the instant period is optimal for oscillators\n",
        "\n",
        "    print(\"Provisioning arrays for dynamic indicators...\")\n",
        "    # Provision arrays for additional indicators\n",
        "    num_data_points = len(data_date)\n",
        "    adx_ht = np.zeros(num_data_points, dtype=float)\n",
        "    aroon_osc_ht = np.zeros(num_data_points, dtype=float)\n",
        "    rsi_ht = np.zeros(num_data_points, dtype=float)\n",
        "    macd_ht = np.zeros(num_data_points, dtype=float)\n",
        "    macdsignal_ht = np.zeros(num_data_points, dtype=float)\n",
        "    macdhist_ht = np.zeros(num_data_points, dtype=float)\n",
        "    slowk_ht = np.zeros(num_data_points, dtype=float)\n",
        "    slowd_ht = np.zeros(num_data_points, dtype=float)\n",
        "    bb_lowerband_ht = np.zeros(num_data_points, dtype=float)\n",
        "    bb_upperband_ht = np.zeros(num_data_points, dtype=float)\n",
        "    bb_pct_ht = np.zeros(num_data_points, dtype=float)\n",
        "    ult_osc_ht = np.zeros(num_data_points, dtype=float)\n",
        "\n",
        "    # Get frequency-adjusted ADX\n",
        "    print(\"Calculating dynamic ADX...\")\n",
        "    i = 0\n",
        "    for period in ht_period:\n",
        "        if not math.isnan(period):\n",
        "            period = int(period)\n",
        "            slowperiod = (\n",
        "                period if period % 2 == 0 else period + 1\n",
        "            )  # Ensure slow period is even\n",
        "            adx = abstract.ADX(\n",
        "                data_high_price, data_low_price, data_close_price, timeperiod=slowperiod\n",
        "            )\n",
        "            adx_ht[i] = adx[i]\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    adx_ht = adx_ht.reshape(-1, 1)\n",
        "    ## Normalize ADX\n",
        "    print(\"Normalizing dynamic ADX data...\")\n",
        "    adx_norm = adx_ht / 100\n",
        "\n",
        "    # Get frequency-adjusted Aroon Oscillator\n",
        "    print(\"Calculating dynamic AROONOSC...\")\n",
        "    i = 0\n",
        "    for period in ht_period:\n",
        "        if not math.isnan(period):\n",
        "            period = int(period)\n",
        "            slowperiod = (\n",
        "                period if period % 2 == 0 else period + 1\n",
        "            )  # Ensure slow period is even\n",
        "            aroon_osc = abstract.AROONOSC(\n",
        "                data_high_price, data_low_price, timeperiod=int(slowperiod/2)\n",
        "            )\n",
        "            aroon_osc_ht[i] = aroon_osc[i]\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    aroon_osc_ht = aroon_osc_ht.reshape(-1, 1)\n",
        "    ## Normalize Aroon Oscillator\n",
        "    print(\"Normalizing dynamic AROONOSC data...\")\n",
        "    aroon_osc_norm = (aroon_osc_ht + 100) / 200  # oscillates between +/- 100\n",
        "\n",
        "    # Get frequency-adjusted MACD\n",
        "    print(\"Calculating dynamic MACD...\")\n",
        "    i = 0\n",
        "    for period in ht_period:\n",
        "        if not math.isnan(period):\n",
        "            period = int(period)\n",
        "            slowperiod = (\n",
        "                period if period % 2 == 0 else period + 1\n",
        "            )  # Ensure slow period is even\n",
        "            fastperiod = int((slowperiod / 2) - 1)\n",
        "            signalperiod = math.floor(0.75 * fastperiod)\n",
        "            macd, macdsignal, macdhist = abstract.MACD(\n",
        "                data_close_price,\n",
        "                fastperiod=fastperiod,\n",
        "                slowperiod=slowperiod,\n",
        "                signalperiod=signalperiod,\n",
        "            )\n",
        "            macd_ht[i] = macd[i]\n",
        "            macdsignal_ht[i] = macdsignal[i]\n",
        "            macdhist_ht[i] = macdhist[i]\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    macdhist_ht = macdhist_ht.reshape(-1, 1)\n",
        "    ## Normalize MACD\n",
        "    print(\"Normalizing dynamic MACD histogram data...\")\n",
        "    macd_scaler = Normalizer()\n",
        "    macd_norm = macd_scaler.fit_transform(macdhist_ht)\n",
        "\n",
        "    # Get frequency-adjusted Bollinger Bands\n",
        "    print(\"Calculating dynamic BBANDS...\")\n",
        "    i = 0\n",
        "    for period in ht_period:\n",
        "        if not math.isnan(period):\n",
        "            period = int(period)\n",
        "            slowperiod = (\n",
        "                period if period % 2 == 0 else period + 1\n",
        "            )  # Ensure slow period is even\n",
        "            bb_upperband, bb_middleband, bb_lowerband = abstract.BBANDS(\n",
        "                data_close_price,\n",
        "                timeperiod=slowperiod,\n",
        "                nbdevup=float(2),\n",
        "                nbdevdn=float(2),\n",
        "                matype=1,\n",
        "            )\n",
        "            bb_upperband_ht[i] = bb_upperband[i]\n",
        "            bb_lowerband_ht[i] = bb_lowerband[i]\n",
        "            bb_pct_ht[i] = (data_close_price[i] - bb_lowerband[i]) / (\n",
        "                bb_upperband[i] - bb_lowerband[i]\n",
        "            )\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    bb_pct_ht = bb_pct_ht.reshape(-1, 1)\n",
        "    ## Normalize MACD\n",
        "    print(\"Normalizing dynamic BBANDS Percentage data...\")\n",
        "    bb_pct_scaler = Normalizer()\n",
        "    bb_pct_norm = bb_pct_scaler.fit_transform(bb_pct_ht)\n",
        "\n",
        "    # Get frequency-adjusted RSI\n",
        "    print(\"Calculating dynamic RSI...\")\n",
        "    i = 0\n",
        "    for period in ht_period:\n",
        "        if not math.isnan(period):\n",
        "            period = (\n",
        "                period / 2 if period % 2 == 0 else (period + 1) / 2\n",
        "            )  # Ensure slow period is even and divide by 2\n",
        "            period = int(period)\n",
        "            rsi = abstract.RSI(\n",
        "                data_close_price,\n",
        "                timeperiod=period,\n",
        "            )\n",
        "            rsi_ht[i] = rsi[i]\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    rsi_ht = rsi_ht.reshape(-1, 1)\n",
        "    ## Normalize RSI\n",
        "    print(\"Normalizing dynamic RSI data...\")\n",
        "    rsi_norm = rsi_ht / 100\n",
        "\n",
        "    # Get frequency-adjusted Stochastic\n",
        "    print(\"Calculating dynamic Stochastic...\")\n",
        "    i = 0\n",
        "    for period in ht_period:\n",
        "        if not math.isnan(period):\n",
        "            period = (\n",
        "                period / 2 if period % 2 == 0 else (period + 1) / 2\n",
        "            )  # Ensure slow period is even and divide by 2\n",
        "            period = int(period)\n",
        "            slowk, slowd = abstract.STOCH(\n",
        "                data_high_price,\n",
        "                data_low_price,\n",
        "                data_close_price,\n",
        "                fastk_period=period,\n",
        "                slowk_period=3,\n",
        "                slowk_matype=1,\n",
        "                slowd_period=3,\n",
        "                slowd_matype=1,\n",
        "            )\n",
        "            slowk_ht[i] = slowk[i]\n",
        "            slowd_ht[i] = slowd[i]\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    stochastic_hist_ht = slowk_ht - slowd_ht\n",
        "    stochastic_hist_ht = stochastic_hist_ht.reshape(-1, 1)\n",
        "    ## Normalize dynamic stochastic histogram\n",
        "    print(\"Normalizing dynamic stochastic histogram data...\")\n",
        "    stochastic_hist_scaler = Normalizer()\n",
        "    stochastic_hist_norm = stochastic_hist_scaler.fit_transform(stochastic_hist_ht)\n",
        "    \n",
        "    # Get frequency-adjusted Ultimate Oscillator\n",
        "    print(\"Calculating dynamic ULTOSC...\")\n",
        "    i = 0\n",
        "    for period in ht_period:\n",
        "        if not math.isnan(period):\n",
        "            period = (\n",
        "                period / 2 if period % 2 == 0 else (period + 1) / 2\n",
        "            )  # Ensure slow period is even and divide by 2\n",
        "            period = int(period)\n",
        "            ult_osc = abstract.ULTOSC(\n",
        "                data_high_price,\n",
        "                data_low_price,\n",
        "                data_close_price,\n",
        "                timeperiod1=period,\n",
        "                timeperiod2=period*2,\n",
        "                timeperiod3=period*4,\n",
        "            )\n",
        "            ult_osc_ht[i] = slowk[i]\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    ult_osc_ht = ult_osc_ht.reshape(-1, 1)\n",
        "    ## Normalize dynamic stochastic histogram\n",
        "    print(\"Normalizing dynamic ULTOSC data...\")\n",
        "    ult_osc_ht = ult_osc_ht / 100 # Naturally ranges between 0-100\n",
        "\n",
        "    data_stack = hstack(\n",
        "        (\n",
        "            # mesa_hist,\n",
        "            ht_trend_sine,\n",
        "            macd_norm,\n",
        "            adx_norm,\n",
        "            aroon_osc_norm,\n",
        "            bb_pct_norm,\n",
        "            rsi_norm,\n",
        "            stochastic_hist_norm,\n",
        "            ult_osc_ht,\n",
        "            data_price_pct_change_norm,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # print(data_stack)\n",
        "    # print(data_stack.size)\n",
        "    data_stack, data_date = remove_nan_columns(data_stack, data_date)\n",
        "    num_data_points = data_stack.shape[0]\n",
        "    display_date_range = (\n",
        "        \"from \" + data_date[0] + \" to \" + data_date[num_data_points - 1]\n",
        "    )\n",
        "    print(\"Number data points:\", num_data_points, display_date_range)\n",
        "\n",
        "    # print(data_stack)\n",
        "    # print(data_stack.size)\n",
        "\n",
        "    return data_date, data_stack, num_data_points, display_date_range, price_pct_change_scaler"
      ],
      "metadata": {
        "id": "BqNjNXDJ-lRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run download_data() Routine"
      ],
      "metadata": {
        "id": "XdSecUL5-qiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_date, data_stack, num_data_points, display_date_range, price_pct_change_scaler = download_data(config)\n",
        "\n",
        "if plot_on:\n",
        "    # # First plot\n",
        "\n",
        "    fig = figure(figsize=(25, 5), dpi=160)\n",
        "    fig.patch.set_facecolor((1.0, 1.0, 1.0))\n",
        "    plt.plot(data_date, data_stack[:, config[\"model\"][\"input_size\"]], color=config[\"plots\"][\"color_actual\"])\n",
        "    xticks = [\n",
        "        data_date[i]\n",
        "        if (\n",
        "            (\n",
        "                i % config[\"plots\"][\"xticks_interval\"] == 0\n",
        "                and (num_data_points - i) > config[\"plots\"][\"xticks_interval\"]\n",
        "            )\n",
        "            or i == num_data_points - 1\n",
        "        )\n",
        "        else None\n",
        "        for i in range(num_data_points)\n",
        "    ]  # make x ticks nice\n",
        "    x = np.arange(0, len(xticks))\n",
        "    plt.xticks(x, xticks, rotation=\"vertical\")\n",
        "    plt.title(\n",
        "        \"Daily close price for \"\n",
        "        + config[\"alpha_vantage\"][\"symbol\"]\n",
        "        + \", \"\n",
        "        + display_date_range\n",
        "    )\n",
        "    plt.grid(b=None, which=\"major\", axis=\"y\", linestyle=\"--\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def prepare_data_x(x, window_size):\n",
        "    # perform windowing\n",
        "    n_row = x.shape[0] - window_size + 1\n",
        "    output = np.lib.stride_tricks.as_strided(\n",
        "        x, shape=(n_row, window_size), strides=(x.strides[0], x.strides[0])\n",
        "    )\n",
        "    return output[:-1], output[-1]\n",
        "\n",
        "\n",
        "def prepare_data_y(x, window_size):\n",
        "    # # perform simple moving average\n",
        "    # output = np.convolve(x, np.ones(window_size), 'valid') / window_size\n",
        "\n",
        "    # use the next day as label\n",
        "    output = x[window_size:]\n",
        "    return output\n",
        "\n",
        "data_x, data_x_unseen = prepare_data_x(\n",
        "    data_stack, window_size=config[\"data\"][\"window_size\"]\n",
        ")\n",
        "# data_unseen = data_unseen.reshape(1,-1)\n",
        "data_y = prepare_data_y(data_stack, window_size=config[\"data\"][\"window_size\"])\n",
        "\n",
        "# split dataset\n",
        "\n",
        "split_index = int(data_y.shape[0] * config[\"data\"][\"train_split_size\"])\n",
        "data_x_train = data_x[:split_index]\n",
        "data_x_val = data_x[split_index:]\n",
        "data_y_train = data_y[:split_index]\n",
        "data_y_val = data_y[split_index:]\n",
        "\n",
        "if plot_on:\n",
        "    # # prepare data for plotting\n",
        "\n",
        "    to_plot_data_y_train = np.zeros(num_data_points)\n",
        "    to_plot_data_y_val = np.zeros(num_data_points)\n",
        "\n",
        "    to_plot_data_y_train[\n",
        "        config[\"data\"][\"window_size\"] : split_index + config[\"data\"][\"window_size\"]\n",
        "    ] = price_pct_change_scaler.inverse_transform(data_y_train[:, -1])\n",
        "    to_plot_data_y_val[\n",
        "        split_index + config[\"data\"][\"window_size\"] :\n",
        "    ] = price_pct_change_scaler.inverse_transform(data_y_val[:, -1])\n",
        "\n",
        "    to_plot_data_y_train = np.where(to_plot_data_y_train == 0, None, to_plot_data_y_train)\n",
        "    to_plot_data_y_val = np.where(to_plot_data_y_val == 0, None, to_plot_data_y_val)\n",
        "\n",
        "    # # plots\n",
        "\n",
        "    fig = figure(figsize=(25, 5), dpi=160)\n",
        "    fig.patch.set_facecolor((1.0, 1.0, 1.0))\n",
        "    plt.plot(\n",
        "        data_date,\n",
        "        to_plot_data_y_train,\n",
        "        label=\"Prices (train)\",\n",
        "        color=config[\"plots\"][\"color_train\"],\n",
        "    )\n",
        "    plt.plot(\n",
        "        data_date,\n",
        "        to_plot_data_y_val,\n",
        "        label=\"Prices (validation)\",\n",
        "        color=config[\"plots\"][\"color_val\"],\n",
        "    )\n",
        "    xticks = [\n",
        "        data_date[i]\n",
        "        if (\n",
        "            (\n",
        "                i % config[\"plots\"][\"xticks_interval\"] == 0\n",
        "                and (num_data_points - i) > config[\"plots\"][\"xticks_interval\"]\n",
        "            )\n",
        "            or i == num_data_points - 1\n",
        "        )\n",
        "        else None\n",
        "        for i in range(num_data_points)\n",
        "    ]  # make x ticks nice\n",
        "    x = np.arange(0, len(xticks))\n",
        "    plt.xticks(x, xticks, rotation=\"vertical\")\n",
        "    plt.title(\n",
        "        \"Daily close prices for \"\n",
        "        + config[\"alpha_vantage\"][\"symbol\"]\n",
        "        + \" - showing training and validation data\"\n",
        "    )\n",
        "    plt.grid(b=None, which=\"major\", axis=\"y\", linestyle=\"--\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, x, target, features, sequence_length=5):\n",
        "        self.features = features\n",
        "        self.target = target\n",
        "        self.sequence_length = sequence_length\n",
        "        if x.ndim > 1:\n",
        "            self.y = torch.tensor(x[:, -1]).float()\n",
        "            self.x = torch.tensor(x[:,:features]).float()\n",
        "        else:\n",
        "            self.y = torch.tensor(x[-1]).float()\n",
        "            self.x = torch.tensor(x[:features]).float()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.x.shape[0]\n",
        "\n",
        "    def __getitem__(self, i): \n",
        "        if i >= self.sequence_length - 1:\n",
        "            i_start = i - self.sequence_length + 1\n",
        "            x = self.x[i_start:(i + 1), :]\n",
        "        else:\n",
        "            padding = self.x[0].repeat(self.sequence_length - i - 1, 1)\n",
        "            x = self.x[0:(i + 1), :]\n",
        "            x = torch.cat((padding, x), 0)\n",
        "\n",
        "        return x, self.y[i]\n",
        "\n",
        "features = config[\"model\"][\"input_size\"]\n",
        "sequence_length = config[\"data\"][\"window_size\"]\n",
        "target = features\n",
        "dataset_train = TimeSeriesDataset(data_x_train, target, features, sequence_length )\n",
        "dataset_val = TimeSeriesDataset(data_x_val, target, features, sequence_length )\n",
        "data_unseen = TimeSeriesDataset(data_x_unseen, target, features, sequence_length )\n",
        "# print(\"Train data shape\", dataset_train.x.size(), dataset_train.y)\n",
        "# print(\"Validation data shape\", dataset_val.x.size(), dataset_val.y)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    dataset_train, batch_size=config[\"training\"][\"batch_size\"], shuffle=True\n",
        ")\n",
        "val_dataloader = DataLoader(\n",
        "    dataset_val, batch_size=config[\"training\"][\"batch_size\"], shuffle=True\n",
        ")\n",
        "\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=8,\n",
        "        hidden_layer_size=32,\n",
        "        num_layers=2,\n",
        "        output_size=1,\n",
        "        dropout=0.2,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.hidden_layer_size = hidden_layer_size\n",
        "        self.linear_1 = nn.Linear(input_size, hidden_layer_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.lstm = nn.LSTM(\n",
        "            hidden_layer_size,\n",
        "            hidden_size=self.hidden_layer_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(num_layers * hidden_layer_size, output_size)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        for name, param in self.lstm.named_parameters():\n",
        "            if \"bias\" in name:\n",
        "                nn.init.constant_(param, 0.0)\n",
        "            elif \"weight_ih\" in name:\n",
        "                nn.init.kaiming_normal_(param)\n",
        "            elif \"weight_hh\" in name:\n",
        "                nn.init.orthogonal_(param)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batchsize = x.shape[0]\n",
        "\n",
        "        # layer 1\n",
        "        x = self.linear_1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        # LSTM layer\n",
        "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
        "\n",
        "        # reshape output from hidden cell into [batch, features] for `linear_2`\n",
        "        x = h_n.permute(1, 0, 2).reshape(batchsize, -1)\n",
        "\n",
        "        # layer 2\n",
        "        x = self.dropout(x)\n",
        "        predictions = self.linear_2(x)\n",
        "        return predictions[:, -1]\n",
        "\n",
        "\n",
        "def run_epoch(dataloader, is_training=False):\n",
        "    epoch_loss = 0\n",
        "\n",
        "    if is_training:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "\n",
        "    for idx, (x, y) in enumerate(dataloader):\n",
        "        if is_training:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        batchsize = x.shape[0]\n",
        "\n",
        "        x = x.to(config[\"training\"][\"device\"])\n",
        "        y = y.to(config[\"training\"][\"device\"])\n",
        "\n",
        "        out = model(x)\n",
        "        loss = criterion(out.contiguous(), y.contiguous())\n",
        "\n",
        "        if is_training:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.detach().item() / batchsize\n",
        "\n",
        "    lr = scheduler.get_last_lr()[0]\n",
        "\n",
        "    return epoch_loss, lr\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    dataset_train, batch_size=config[\"training\"][\"batch_size\"], shuffle=True\n",
        ")\n",
        "val_dataloader = DataLoader(\n",
        "    dataset_val, batch_size=config[\"training\"][\"batch_size\"], shuffle=True\n",
        ")\n",
        "\n",
        "model = LSTMModel(\n",
        "    input_size=config[\"model\"][\"input_size\"],\n",
        "    hidden_layer_size=config[\"model\"][\"lstm_size\"],\n",
        "    num_layers=config[\"model\"][\"num_lstm_layers\"],\n",
        "    output_size=1,\n",
        "    dropout=config[\"model\"][\"dropout\"],\n",
        ")\n",
        "model = model.to(config[\"training\"][\"device\"])\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=config[\"training\"][\"learning_rate\"],\n",
        "    betas=(0.9, 0.98),\n",
        "    eps=1e-9,\n",
        ")\n",
        "scheduler = optim.lr_scheduler.StepLR(\n",
        "    optimizer, step_size=config[\"training\"][\"scheduler_step_size\"], gamma=0.1\n",
        ")\n",
        "\n",
        "for epoch in range(config[\"training\"][\"num_epoch\"]):\n",
        "    loss_train, lr_train = run_epoch(train_dataloader, is_training=True)\n",
        "    loss_val, lr_val = run_epoch(val_dataloader)\n",
        "    scheduler.step()\n",
        "\n",
        "    print(\n",
        "        \"Epoch[{}/{}] | loss train:{:.6f}, test:{:.6f} | lr:{:.6f}\".format(\n",
        "            epoch + 1, config[\"training\"][\"num_epoch\"], loss_train, loss_val, lr_train\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "# here we re-initialize dataloader so the data doesn't shuffled, so we can plot the values by date\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    dataset_train, batch_size=config[\"training\"][\"batch_size\"], shuffle=False\n",
        ")\n",
        "val_dataloader = DataLoader(\n",
        "    dataset_val, batch_size=config[\"training\"][\"batch_size\"], shuffle=False\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# predict on the training data, to see how well the model managed to learn and memorize\n",
        "\n",
        "predicted_train = np.array([])\n",
        "\n",
        "for idx, (x, y) in enumerate(train_dataloader):\n",
        "    x = x.to(config[\"training\"][\"device\"])\n",
        "    out = model(x)\n",
        "    out = out.cpu().detach().numpy()\n",
        "    predicted_train = np.concatenate((predicted_train, out))\n",
        "\n",
        "# predict on the validation data, to see how the model does\n",
        "\n",
        "predicted_val = np.array([])\n",
        "\n",
        "for idx, (x, y) in enumerate(val_dataloader):\n",
        "    x = x.to(config[\"training\"][\"device\"])\n",
        "    out = model(x)\n",
        "    out = out.cpu().detach().numpy()\n",
        "    predicted_val = np.concatenate((predicted_val, out))\n",
        "\n",
        "# prepare data for plotting\n",
        "\n",
        "to_plot_data_y_train_pred = np.zeros(num_data_points)\n",
        "to_plot_data_y_val_pred = np.zeros(num_data_points)\n",
        "\n",
        "predicted_train = predicted_train.reshape(1, -1)\n",
        "predicted_val = predicted_val.reshape(1, -1)\n",
        "\n",
        "to_plot_data_y_train_pred[\n",
        "    config[\"data\"][\"window_size\"] : split_index + config[\"data\"][\"window_size\"]\n",
        "] = price_pct_change_scaler.inverse_transform(predicted_train)\n",
        "to_plot_data_y_val_pred[\n",
        "    split_index + config[\"data\"][\"window_size\"] :\n",
        "] = price_pct_change_scaler.inverse_transform(predicted_val)\n",
        "\n",
        "to_plot_data_y_train_pred = np.where(\n",
        "    to_plot_data_y_train_pred == 0, None, to_plot_data_y_train_pred\n",
        ")\n",
        "to_plot_data_y_val_pred = np.where(\n",
        "    to_plot_data_y_val_pred == 0, None, to_plot_data_y_val_pred\n",
        ")\n",
        "\n",
        "\n",
        "if plot_on:\n",
        "    # # plots\n",
        "\n",
        "    fig = figure(figsize=(25, 5), dpi=80)\n",
        "    fig.patch.set_facecolor((1.0, 1.0, 1.0))\n",
        "    plt.plot(\n",
        "        data_date,\n",
        "        data_stack[:, config[\"model\"][\"input_size\"]], #Actual data\n",
        "        label=\"Actual prices\",\n",
        "        color=config[\"plots\"][\"color_actual\"],\n",
        "    )\n",
        "    plt.plot(\n",
        "        data_date,\n",
        "        to_plot_data_y_train_pred,\n",
        "        label=\"Predicted prices (train)\",\n",
        "        color=config[\"plots\"][\"color_pred_train\"],\n",
        "    )\n",
        "    plt.plot(\n",
        "        data_date,\n",
        "        to_plot_data_y_val_pred,\n",
        "        label=\"Predicted prices (validation)\",\n",
        "        color=config[\"plots\"][\"color_pred_val\"],\n",
        "    )\n",
        "    plt.title(\"Compare predicted prices to actual prices\")\n",
        "    xticks = [\n",
        "        data_date[i]\n",
        "        if (\n",
        "            (\n",
        "                i % config[\"plots\"][\"xticks_interval\"] == 0\n",
        "                and (num_data_points - i) > config[\"plots\"][\"xticks_interval\"]\n",
        "            )\n",
        "            or i == num_data_points - 1\n",
        "        )\n",
        "        else None\n",
        "        for i in range(num_data_points)\n",
        "    ]  # make x ticks nice\n",
        "    x = np.arange(0, len(xticks))\n",
        "    plt.xticks(x, xticks, rotation=\"vertical\")\n",
        "    plt.grid(b=None, which=\"major\", axis=\"y\", linestyle=\"--\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # prepare data for plotting the zoomed in view of the predicted prices vs. actual prices\n",
        "\n",
        "    to_plot_data_y_val_subset = price_pct_change_scaler.inverse_transform(data_y_val)\n",
        "    to_plot_predicted_val = price_pct_change_scaler.inverse_transform(predicted_val)\n",
        "    to_plot_data_date = data_date[split_index + config[\"data\"][\"window_size\"] :]\n",
        "\n",
        "\n",
        "    # plots\n",
        "\n",
        "    fig = figure(figsize=(25, 5), dpi=80)\n",
        "    fig.patch.set_facecolor((1.0, 1.0, 1.0))\n",
        "    plt.plot(\n",
        "        to_plot_data_date,\n",
        "        to_plot_data_y_val_subset,\n",
        "        label=\"Actual prices\",\n",
        "        color=config[\"plots\"][\"color_actual\"],\n",
        "    )\n",
        "    plt.plot(\n",
        "        to_plot_data_date,\n",
        "        to_plot_predicted_val[:, -1],\n",
        "        label=\"Predicted prices (validation)\",\n",
        "        color=config[\"plots\"][\"color_pred_val\"],\n",
        "    )\n",
        "    plt.title(\"Zoom in to examine predicted price on validation data portion\")\n",
        "    xticks = [\n",
        "        to_plot_data_date[i]\n",
        "        if (\n",
        "            (\n",
        "                i % int(config[\"plots\"][\"xticks_interval\"] / 5) == 0\n",
        "                and (len(to_plot_data_date) - i) > config[\"plots\"][\"xticks_interval\"] / 6\n",
        "            )\n",
        "            or i == len(to_plot_data_date) - 1\n",
        "        )\n",
        "        else None\n",
        "        for i in range(len(to_plot_data_date))\n",
        "    ]  # make x ticks nice\n",
        "    xs = np.arange(0, len(xticks))\n",
        "    plt.xticks(xs, xticks, rotation=\"vertical\")\n",
        "    plt.grid(b=None, which=\"major\", axis=\"y\", linestyle=\"--\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# predict the closing price of the next trading day\n",
        "\n",
        "model.eval()\n",
        "\n",
        "x = (\n",
        "    torch.tensor(data_x_unseen)\n",
        "    .float()\n",
        "    .to(config[\"training\"][\"device\"])\n",
        "    .unsqueeze(0)\n",
        "    .unsqueeze(2)\n",
        ")  # this is the data type and shape required, [batch, sequence, feature]\n",
        "prediction = model(x)\n",
        "prediction = prediction.cpu().detach().numpy()\n",
        "\n",
        "# prepare plots\n",
        "\n",
        "plot_range = 10\n",
        "# to_plot_data_y_val = np.zeros((plot_range, plot_range-1))\n",
        "# to_plot_data_y_val_pred = np.zeros((plot_range, plot_range-1))\n",
        "# to_plot_data_y_test_pred = np.zeros((plot_range, plot_range-1))\n",
        "\n",
        "to_plot_data_y_val = price_pct_change_scaler.inverse_transform(data_y_val)[\n",
        "    -plot_range + 1 :\n",
        "]\n",
        "to_plot_data_y_val_pred = price_pct_change_scaler.inverse_transform(predicted_val)[\n",
        "    -plot_range + 1 :\n",
        "]\n",
        "\n",
        "to_plot_data_y_test_pred = price_pct_change_scaler.inverse_transform(prediction)\n",
        "\n",
        "to_plot_data_y_val = np.where(to_plot_data_y_val == 0, None, to_plot_data_y_val)\n",
        "to_plot_data_y_val_pred = np.where(\n",
        "    to_plot_data_y_val_pred == 0, None, to_plot_data_y_val_pred\n",
        ")\n",
        "to_plot_data_y_test_pred = np.where(\n",
        "    to_plot_data_y_test_pred == 0, None, to_plot_data_y_test_pred\n",
        ")\n",
        "\n",
        "# plot\n",
        "if plot_on:\n",
        "    plot_date_test = data_date[-plot_range + 1 :]\n",
        "    plot_date_test.append(\"tomorrow\")\n",
        "\n",
        "    fig = figure(figsize=(25, 5), dpi=80)\n",
        "    fig.patch.set_facecolor((1.0, 1.0, 1.0))\n",
        "    plt.plot(\n",
        "        plot_date_test,\n",
        "        to_plot_data_y_val,\n",
        "        label=\"Actual prices\",\n",
        "        marker=\".\",\n",
        "        markersize=10,\n",
        "        color=config[\"plots\"][\"color_actual\"],\n",
        "    )\n",
        "    plt.plot(\n",
        "        plot_date_test,\n",
        "        to_plot_data_y_val_pred,\n",
        "        label=\"Past predicted prices\",\n",
        "        marker=\".\",\n",
        "        markersize=10,\n",
        "        color=config[\"plots\"][\"color_pred_val\"],\n",
        "    )\n",
        "    plt.plot(\n",
        "        plot_date_test,\n",
        "        to_plot_data_y_test_pred,\n",
        "        label=\"Predicted price for next day\",\n",
        "        marker=\".\",\n",
        "        markersize=20,\n",
        "        color=config[\"plots\"][\"color_pred_test\"],\n",
        "    )\n",
        "    plt.title(\"Predicting the close price of the next trading day\")\n",
        "    plt.grid(b=None, which=\"major\", axis=\"y\", linestyle=\"--\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "print(\n",
        "    \"Predicted close price of the next trading day:\",\n",
        "    to_plot_data_y_test_pred\n",
        ")"
      ],
      "metadata": {
        "id": "E6FKdfXM1GhG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1a07cec5-debb-4a51-b04f-0e74a17d8140"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining data for symbol: TQQQ\n",
            "Sorting data...\n",
            "Sorting close price...\n",
            "Normalizing price percent change data...\n",
            "Sorting high prices...\n",
            "Sorting low prices...\n",
            "Calculating HT_TRENDLINE...\n",
            "Normalizing HT_TRENDLINE data...\n",
            "Calculating MESA average price...\n",
            "Normalizing dynamic MACD histogram data...\n",
            "Calculating HT_TREND sine signals...\n",
            "Normalizing dynamic MACD histogram data...\n",
            "Calculating HT_PERIODS for dynamic indicators...\n",
            "Provisioning arrays for dynamic indicators...\n",
            "Calculating dynamic ADX...\n",
            "Normalizing dynamic ADX data...\n",
            "Calculating dynamic AROONOSC...\n",
            "Normalizing dynamic AROONOSC data...\n",
            "Calculating dynamic MACD...\n",
            "Normalizing dynamic MACD histogram data...\n",
            "Calculating dynamic BBANDS...\n",
            "Normalizing dynamic BBANDS Percentage data...\n",
            "Calculating dynamic RSI...\n",
            "Normalizing dynamic RSI data...\n",
            "Calculating dynamic Stochastic...\n",
            "Normalizing dynamic stochastic histogram data...\n",
            "Calculating dynamic ULTOSC...\n",
            "Normalizing dynamic ULTOSC data...\n",
            "Removing 67 nan rows from data stack...\n",
            "Number data points: 3186 from 2010-05-19 to 2023-01-12\n",
            "Epoch[1/300] | loss train:0.102862, test:0.009137 | lr:0.010000\n",
            "Epoch[2/300] | loss train:0.046024, test:0.009199 | lr:0.010000\n",
            "Epoch[3/300] | loss train:0.043663, test:0.009032 | lr:0.010000\n",
            "Epoch[4/300] | loss train:0.041646, test:0.009037 | lr:0.010000\n",
            "Epoch[5/300] | loss train:0.039972, test:0.008900 | lr:0.010000\n",
            "Epoch[6/300] | loss train:0.039103, test:0.008185 | lr:0.010000\n",
            "Epoch[7/300] | loss train:0.035454, test:0.007075 | lr:0.010000\n",
            "Epoch[8/300] | loss train:0.035322, test:0.008046 | lr:0.010000\n",
            "Epoch[9/300] | loss train:0.034394, test:0.008936 | lr:0.010000\n",
            "Epoch[10/300] | loss train:0.031238, test:0.006943 | lr:0.010000\n",
            "Epoch[11/300] | loss train:0.029827, test:0.009208 | lr:0.010000\n",
            "Epoch[12/300] | loss train:0.029998, test:0.007283 | lr:0.010000\n",
            "Epoch[13/300] | loss train:0.028148, test:0.006938 | lr:0.010000\n",
            "Epoch[14/300] | loss train:0.027009, test:0.006758 | lr:0.010000\n",
            "Epoch[15/300] | loss train:0.025830, test:0.008038 | lr:0.010000\n",
            "Epoch[16/300] | loss train:0.024273, test:0.008289 | lr:0.010000\n",
            "Epoch[17/300] | loss train:0.024631, test:0.007545 | lr:0.010000\n",
            "Epoch[18/300] | loss train:0.023814, test:0.007159 | lr:0.010000\n",
            "Epoch[19/300] | loss train:0.023224, test:0.007616 | lr:0.010000\n",
            "Epoch[20/300] | loss train:0.021975, test:0.008805 | lr:0.010000\n",
            "Epoch[21/300] | loss train:0.021252, test:0.007785 | lr:0.010000\n",
            "Epoch[22/300] | loss train:0.020561, test:0.007893 | lr:0.010000\n",
            "Epoch[23/300] | loss train:0.020143, test:0.008380 | lr:0.010000\n",
            "Epoch[24/300] | loss train:0.018617, test:0.007411 | lr:0.010000\n",
            "Epoch[25/300] | loss train:0.018518, test:0.009098 | lr:0.010000\n",
            "Epoch[26/300] | loss train:0.017246, test:0.007296 | lr:0.010000\n",
            "Epoch[27/300] | loss train:0.017026, test:0.008944 | lr:0.010000\n",
            "Epoch[28/300] | loss train:0.018910, test:0.008284 | lr:0.010000\n",
            "Epoch[29/300] | loss train:0.016369, test:0.008160 | lr:0.010000\n",
            "Epoch[30/300] | loss train:0.015216, test:0.008059 | lr:0.010000\n",
            "Epoch[31/300] | loss train:0.016967, test:0.008405 | lr:0.010000\n",
            "Epoch[32/300] | loss train:0.015657, test:0.008196 | lr:0.010000\n",
            "Epoch[33/300] | loss train:0.015027, test:0.008534 | lr:0.010000\n",
            "Epoch[34/300] | loss train:0.013248, test:0.007600 | lr:0.010000\n",
            "Epoch[35/300] | loss train:0.013568, test:0.008041 | lr:0.010000\n",
            "Epoch[36/300] | loss train:0.012800, test:0.009043 | lr:0.010000\n",
            "Epoch[37/300] | loss train:0.013848, test:0.007789 | lr:0.010000\n",
            "Epoch[38/300] | loss train:0.013127, test:0.009142 | lr:0.010000\n",
            "Epoch[39/300] | loss train:0.011954, test:0.009227 | lr:0.010000\n",
            "Epoch[40/300] | loss train:0.011645, test:0.009037 | lr:0.010000\n",
            "Epoch[41/300] | loss train:0.008946, test:0.008975 | lr:0.001000\n",
            "Epoch[42/300] | loss train:0.007725, test:0.008926 | lr:0.001000\n",
            "Epoch[43/300] | loss train:0.007030, test:0.009071 | lr:0.001000\n",
            "Epoch[44/300] | loss train:0.006829, test:0.009181 | lr:0.001000\n",
            "Epoch[45/300] | loss train:0.006641, test:0.009515 | lr:0.001000\n",
            "Epoch[46/300] | loss train:0.006348, test:0.009208 | lr:0.001000\n",
            "Epoch[47/300] | loss train:0.005987, test:0.009131 | lr:0.001000\n",
            "Epoch[48/300] | loss train:0.006027, test:0.009275 | lr:0.001000\n",
            "Epoch[49/300] | loss train:0.005842, test:0.009386 | lr:0.001000\n",
            "Epoch[50/300] | loss train:0.005577, test:0.009226 | lr:0.001000\n",
            "Epoch[51/300] | loss train:0.005444, test:0.009352 | lr:0.001000\n",
            "Epoch[52/300] | loss train:0.005244, test:0.009110 | lr:0.001000\n",
            "Epoch[53/300] | loss train:0.005030, test:0.009601 | lr:0.001000\n",
            "Epoch[54/300] | loss train:0.005075, test:0.009504 | lr:0.001000\n",
            "Epoch[55/300] | loss train:0.004847, test:0.009420 | lr:0.001000\n",
            "Epoch[56/300] | loss train:0.004789, test:0.009557 | lr:0.001000\n",
            "Epoch[57/300] | loss train:0.004808, test:0.009403 | lr:0.001000\n",
            "Epoch[58/300] | loss train:0.004451, test:0.009754 | lr:0.001000\n",
            "Epoch[59/300] | loss train:0.004491, test:0.009273 | lr:0.001000\n",
            "Epoch[60/300] | loss train:0.004477, test:0.009614 | lr:0.001000\n",
            "Epoch[61/300] | loss train:0.004545, test:0.009757 | lr:0.001000\n",
            "Epoch[62/300] | loss train:0.004418, test:0.009602 | lr:0.001000\n",
            "Epoch[63/300] | loss train:0.004229, test:0.009570 | lr:0.001000\n",
            "Epoch[64/300] | loss train:0.004111, test:0.009442 | lr:0.001000\n",
            "Epoch[65/300] | loss train:0.004140, test:0.009464 | lr:0.001000\n",
            "Epoch[66/300] | loss train:0.004133, test:0.009804 | lr:0.001000\n",
            "Epoch[67/300] | loss train:0.004090, test:0.009534 | lr:0.001000\n",
            "Epoch[68/300] | loss train:0.004097, test:0.009743 | lr:0.001000\n",
            "Epoch[69/300] | loss train:0.004017, test:0.009809 | lr:0.001000\n",
            "Epoch[70/300] | loss train:0.003801, test:0.009876 | lr:0.001000\n",
            "Epoch[71/300] | loss train:0.003795, test:0.009663 | lr:0.001000\n",
            "Epoch[72/300] | loss train:0.003760, test:0.009743 | lr:0.001000\n",
            "Epoch[73/300] | loss train:0.003640, test:0.009824 | lr:0.001000\n",
            "Epoch[74/300] | loss train:0.003624, test:0.009819 | lr:0.001000\n",
            "Epoch[75/300] | loss train:0.003763, test:0.009767 | lr:0.001000\n",
            "Epoch[76/300] | loss train:0.003456, test:0.009671 | lr:0.001000\n",
            "Epoch[77/300] | loss train:0.003452, test:0.010160 | lr:0.001000\n",
            "Epoch[78/300] | loss train:0.003498, test:0.010052 | lr:0.001000\n",
            "Epoch[79/300] | loss train:0.003391, test:0.009815 | lr:0.001000\n",
            "Epoch[80/300] | loss train:0.003284, test:0.010056 | lr:0.001000\n",
            "Epoch[81/300] | loss train:0.003153, test:0.009916 | lr:0.000100\n",
            "Epoch[82/300] | loss train:0.003114, test:0.009959 | lr:0.000100\n",
            "Epoch[83/300] | loss train:0.003119, test:0.010041 | lr:0.000100\n",
            "Epoch[84/300] | loss train:0.003045, test:0.009989 | lr:0.000100\n",
            "Epoch[85/300] | loss train:0.002980, test:0.010021 | lr:0.000100\n",
            "Epoch[86/300] | loss train:0.002953, test:0.010003 | lr:0.000100\n",
            "Epoch[87/300] | loss train:0.002935, test:0.010007 | lr:0.000100\n",
            "Epoch[88/300] | loss train:0.003025, test:0.010086 | lr:0.000100\n",
            "Epoch[89/300] | loss train:0.002892, test:0.009984 | lr:0.000100\n",
            "Epoch[90/300] | loss train:0.002866, test:0.009874 | lr:0.000100\n",
            "Epoch[91/300] | loss train:0.002957, test:0.010020 | lr:0.000100\n",
            "Epoch[92/300] | loss train:0.002884, test:0.009888 | lr:0.000100\n",
            "Epoch[93/300] | loss train:0.002986, test:0.010014 | lr:0.000100\n",
            "Epoch[94/300] | loss train:0.002776, test:0.009893 | lr:0.000100\n",
            "Epoch[95/300] | loss train:0.002876, test:0.009950 | lr:0.000100\n",
            "Epoch[96/300] | loss train:0.002877, test:0.009989 | lr:0.000100\n",
            "Epoch[97/300] | loss train:0.002827, test:0.010027 | lr:0.000100\n",
            "Epoch[98/300] | loss train:0.002793, test:0.010029 | lr:0.000100\n",
            "Epoch[99/300] | loss train:0.002838, test:0.009923 | lr:0.000100\n",
            "Epoch[100/300] | loss train:0.002784, test:0.009902 | lr:0.000100\n",
            "Epoch[101/300] | loss train:0.002875, test:0.009981 | lr:0.000100\n",
            "Epoch[102/300] | loss train:0.002789, test:0.009916 | lr:0.000100\n",
            "Epoch[103/300] | loss train:0.002713, test:0.009862 | lr:0.000100\n",
            "Epoch[104/300] | loss train:0.002872, test:0.009999 | lr:0.000100\n",
            "Epoch[105/300] | loss train:0.002832, test:0.009977 | lr:0.000100\n",
            "Epoch[106/300] | loss train:0.002739, test:0.010001 | lr:0.000100\n",
            "Epoch[107/300] | loss train:0.002780, test:0.009992 | lr:0.000100\n",
            "Epoch[108/300] | loss train:0.002829, test:0.010031 | lr:0.000100\n",
            "Epoch[109/300] | loss train:0.002747, test:0.009985 | lr:0.000100\n",
            "Epoch[110/300] | loss train:0.002748, test:0.009935 | lr:0.000100\n",
            "Epoch[111/300] | loss train:0.002703, test:0.009912 | lr:0.000100\n",
            "Epoch[112/300] | loss train:0.002812, test:0.010016 | lr:0.000100\n",
            "Epoch[113/300] | loss train:0.002724, test:0.009928 | lr:0.000100\n",
            "Epoch[114/300] | loss train:0.002779, test:0.010005 | lr:0.000100\n",
            "Epoch[115/300] | loss train:0.002760, test:0.009949 | lr:0.000100\n",
            "Epoch[116/300] | loss train:0.002812, test:0.009901 | lr:0.000100\n",
            "Epoch[117/300] | loss train:0.002666, test:0.009989 | lr:0.000100\n",
            "Epoch[118/300] | loss train:0.002691, test:0.009962 | lr:0.000100\n",
            "Epoch[119/300] | loss train:0.002817, test:0.010014 | lr:0.000100\n",
            "Epoch[120/300] | loss train:0.002677, test:0.009995 | lr:0.000100\n",
            "Epoch[121/300] | loss train:0.002663, test:0.009979 | lr:0.000010\n",
            "Epoch[122/300] | loss train:0.002647, test:0.009942 | lr:0.000010\n",
            "Epoch[123/300] | loss train:0.002707, test:0.009970 | lr:0.000010\n",
            "Epoch[124/300] | loss train:0.002787, test:0.009908 | lr:0.000010\n",
            "Epoch[125/300] | loss train:0.002650, test:0.009948 | lr:0.000010\n",
            "Epoch[126/300] | loss train:0.002847, test:0.009961 | lr:0.000010\n",
            "Epoch[127/300] | loss train:0.002726, test:0.009949 | lr:0.000010\n",
            "Epoch[128/300] | loss train:0.002770, test:0.009979 | lr:0.000010\n",
            "Epoch[129/300] | loss train:0.002689, test:0.009967 | lr:0.000010\n",
            "Epoch[130/300] | loss train:0.002629, test:0.009930 | lr:0.000010\n",
            "Epoch[131/300] | loss train:0.002602, test:0.009969 | lr:0.000010\n",
            "Epoch[132/300] | loss train:0.002770, test:0.009998 | lr:0.000010\n",
            "Epoch[133/300] | loss train:0.002705, test:0.010022 | lr:0.000010\n",
            "Epoch[134/300] | loss train:0.002701, test:0.009988 | lr:0.000010\n",
            "Epoch[135/300] | loss train:0.002628, test:0.009998 | lr:0.000010\n",
            "Epoch[136/300] | loss train:0.002796, test:0.010016 | lr:0.000010\n",
            "Epoch[137/300] | loss train:0.002648, test:0.009996 | lr:0.000010\n",
            "Epoch[138/300] | loss train:0.002728, test:0.009999 | lr:0.000010\n",
            "Epoch[139/300] | loss train:0.002726, test:0.010094 | lr:0.000010\n",
            "Epoch[140/300] | loss train:0.002673, test:0.009981 | lr:0.000010\n",
            "Epoch[141/300] | loss train:0.002756, test:0.009974 | lr:0.000010\n",
            "Epoch[142/300] | loss train:0.002675, test:0.009987 | lr:0.000010\n",
            "Epoch[143/300] | loss train:0.002706, test:0.010106 | lr:0.000010\n",
            "Epoch[144/300] | loss train:0.002774, test:0.010055 | lr:0.000010\n",
            "Epoch[145/300] | loss train:0.002608, test:0.010101 | lr:0.000010\n",
            "Epoch[146/300] | loss train:0.002628, test:0.010026 | lr:0.000010\n",
            "Epoch[147/300] | loss train:0.002608, test:0.010012 | lr:0.000010\n",
            "Epoch[148/300] | loss train:0.002714, test:0.010096 | lr:0.000010\n",
            "Epoch[149/300] | loss train:0.002681, test:0.010054 | lr:0.000010\n",
            "Epoch[150/300] | loss train:0.002729, test:0.010031 | lr:0.000010\n",
            "Epoch[151/300] | loss train:0.002681, test:0.009969 | lr:0.000010\n",
            "Epoch[152/300] | loss train:0.002726, test:0.009998 | lr:0.000010\n",
            "Epoch[153/300] | loss train:0.002704, test:0.010001 | lr:0.000010\n",
            "Epoch[154/300] | loss train:0.002684, test:0.010106 | lr:0.000010\n",
            "Epoch[155/300] | loss train:0.002683, test:0.010017 | lr:0.000010\n",
            "Epoch[156/300] | loss train:0.002609, test:0.009982 | lr:0.000010\n",
            "Epoch[157/300] | loss train:0.002656, test:0.009983 | lr:0.000010\n",
            "Epoch[158/300] | loss train:0.002716, test:0.009977 | lr:0.000010\n",
            "Epoch[159/300] | loss train:0.002755, test:0.010057 | lr:0.000010\n",
            "Epoch[160/300] | loss train:0.002619, test:0.010033 | lr:0.000010\n",
            "Epoch[161/300] | loss train:0.002664, test:0.009986 | lr:0.000001\n",
            "Epoch[162/300] | loss train:0.002771, test:0.010039 | lr:0.000001\n",
            "Epoch[163/300] | loss train:0.002648, test:0.010055 | lr:0.000001\n",
            "Epoch[164/300] | loss train:0.002620, test:0.010002 | lr:0.000001\n",
            "Epoch[165/300] | loss train:0.002645, test:0.010014 | lr:0.000001\n",
            "Epoch[166/300] | loss train:0.002576, test:0.010020 | lr:0.000001\n",
            "Epoch[167/300] | loss train:0.002709, test:0.010014 | lr:0.000001\n",
            "Epoch[168/300] | loss train:0.002697, test:0.009972 | lr:0.000001\n",
            "Epoch[169/300] | loss train:0.002683, test:0.010052 | lr:0.000001\n",
            "Epoch[170/300] | loss train:0.002730, test:0.010013 | lr:0.000001\n",
            "Epoch[171/300] | loss train:0.002623, test:0.010009 | lr:0.000001\n",
            "Epoch[172/300] | loss train:0.002539, test:0.010018 | lr:0.000001\n",
            "Epoch[173/300] | loss train:0.002673, test:0.010024 | lr:0.000001\n",
            "Epoch[174/300] | loss train:0.002569, test:0.010044 | lr:0.000001\n",
            "Epoch[175/300] | loss train:0.002710, test:0.009977 | lr:0.000001\n",
            "Epoch[176/300] | loss train:0.002732, test:0.010028 | lr:0.000001\n",
            "Epoch[177/300] | loss train:0.002776, test:0.010068 | lr:0.000001\n",
            "Epoch[178/300] | loss train:0.002593, test:0.010060 | lr:0.000001\n",
            "Epoch[179/300] | loss train:0.002701, test:0.010018 | lr:0.000001\n",
            "Epoch[180/300] | loss train:0.002624, test:0.010044 | lr:0.000001\n",
            "Epoch[181/300] | loss train:0.002619, test:0.010004 | lr:0.000001\n",
            "Epoch[182/300] | loss train:0.002665, test:0.010053 | lr:0.000001\n",
            "Epoch[183/300] | loss train:0.002575, test:0.010080 | lr:0.000001\n",
            "Epoch[184/300] | loss train:0.002786, test:0.010082 | lr:0.000001\n",
            "Epoch[185/300] | loss train:0.002732, test:0.010069 | lr:0.000001\n",
            "Epoch[186/300] | loss train:0.002673, test:0.010023 | lr:0.000001\n",
            "Epoch[187/300] | loss train:0.002763, test:0.010003 | lr:0.000001\n",
            "Epoch[188/300] | loss train:0.002625, test:0.010054 | lr:0.000001\n",
            "Epoch[189/300] | loss train:0.002633, test:0.009976 | lr:0.000001\n",
            "Epoch[190/300] | loss train:0.002673, test:0.010131 | lr:0.000001\n",
            "Epoch[191/300] | loss train:0.002710, test:0.010000 | lr:0.000001\n",
            "Epoch[192/300] | loss train:0.002650, test:0.010042 | lr:0.000001\n",
            "Epoch[193/300] | loss train:0.002698, test:0.010014 | lr:0.000001\n",
            "Epoch[194/300] | loss train:0.002724, test:0.010104 | lr:0.000001\n",
            "Epoch[195/300] | loss train:0.002667, test:0.010033 | lr:0.000001\n",
            "Epoch[196/300] | loss train:0.002689, test:0.010014 | lr:0.000001\n",
            "Epoch[197/300] | loss train:0.002626, test:0.010052 | lr:0.000001\n",
            "Epoch[198/300] | loss train:0.002649, test:0.009994 | lr:0.000001\n",
            "Epoch[199/300] | loss train:0.002758, test:0.010053 | lr:0.000001\n",
            "Epoch[200/300] | loss train:0.002693, test:0.009999 | lr:0.000001\n",
            "Epoch[201/300] | loss train:0.002672, test:0.010011 | lr:0.000000\n",
            "Epoch[202/300] | loss train:0.002652, test:0.010044 | lr:0.000000\n",
            "Epoch[203/300] | loss train:0.002744, test:0.010069 | lr:0.000000\n",
            "Epoch[204/300] | loss train:0.002701, test:0.010110 | lr:0.000000\n",
            "Epoch[205/300] | loss train:0.002619, test:0.010033 | lr:0.000000\n",
            "Epoch[206/300] | loss train:0.002722, test:0.010031 | lr:0.000000\n",
            "Epoch[207/300] | loss train:0.002574, test:0.010051 | lr:0.000000\n",
            "Epoch[208/300] | loss train:0.002633, test:0.010006 | lr:0.000000\n",
            "Epoch[209/300] | loss train:0.002654, test:0.010084 | lr:0.000000\n",
            "Epoch[210/300] | loss train:0.002654, test:0.010099 | lr:0.000000\n",
            "Epoch[211/300] | loss train:0.002671, test:0.010045 | lr:0.000000\n",
            "Epoch[212/300] | loss train:0.002746, test:0.010074 | lr:0.000000\n",
            "Epoch[213/300] | loss train:0.002663, test:0.010021 | lr:0.000000\n",
            "Epoch[214/300] | loss train:0.002661, test:0.010055 | lr:0.000000\n",
            "Epoch[215/300] | loss train:0.002533, test:0.010121 | lr:0.000000\n",
            "Epoch[216/300] | loss train:0.002667, test:0.010039 | lr:0.000000\n",
            "Epoch[217/300] | loss train:0.002678, test:0.010157 | lr:0.000000\n",
            "Epoch[218/300] | loss train:0.002757, test:0.010017 | lr:0.000000\n",
            "Epoch[219/300] | loss train:0.002630, test:0.010029 | lr:0.000000\n",
            "Epoch[220/300] | loss train:0.002649, test:0.010091 | lr:0.000000\n",
            "Epoch[221/300] | loss train:0.002686, test:0.010005 | lr:0.000000\n",
            "Epoch[222/300] | loss train:0.002657, test:0.010002 | lr:0.000000\n",
            "Epoch[223/300] | loss train:0.002744, test:0.010069 | lr:0.000000\n",
            "Epoch[224/300] | loss train:0.002652, test:0.010025 | lr:0.000000\n",
            "Epoch[225/300] | loss train:0.002686, test:0.010041 | lr:0.000000\n",
            "Epoch[226/300] | loss train:0.002605, test:0.010021 | lr:0.000000\n",
            "Epoch[227/300] | loss train:0.002644, test:0.010054 | lr:0.000000\n",
            "Epoch[228/300] | loss train:0.002742, test:0.010048 | lr:0.000000\n",
            "Epoch[229/300] | loss train:0.002670, test:0.009957 | lr:0.000000\n",
            "Epoch[230/300] | loss train:0.002588, test:0.009981 | lr:0.000000\n",
            "Epoch[231/300] | loss train:0.002610, test:0.010008 | lr:0.000000\n",
            "Epoch[232/300] | loss train:0.002714, test:0.009973 | lr:0.000000\n",
            "Epoch[233/300] | loss train:0.002632, test:0.010026 | lr:0.000000\n",
            "Epoch[234/300] | loss train:0.002642, test:0.010011 | lr:0.000000\n",
            "Epoch[235/300] | loss train:0.002679, test:0.010029 | lr:0.000000\n",
            "Epoch[236/300] | loss train:0.002709, test:0.009999 | lr:0.000000\n",
            "Epoch[237/300] | loss train:0.002621, test:0.009996 | lr:0.000000\n",
            "Epoch[238/300] | loss train:0.002630, test:0.009997 | lr:0.000000\n",
            "Epoch[239/300] | loss train:0.002686, test:0.010068 | lr:0.000000\n",
            "Epoch[240/300] | loss train:0.002637, test:0.010039 | lr:0.000000\n",
            "Epoch[241/300] | loss train:0.002669, test:0.010074 | lr:0.000000\n",
            "Epoch[242/300] | loss train:0.002642, test:0.010014 | lr:0.000000\n",
            "Epoch[243/300] | loss train:0.002691, test:0.010012 | lr:0.000000\n",
            "Epoch[244/300] | loss train:0.002708, test:0.010014 | lr:0.000000\n",
            "Epoch[245/300] | loss train:0.002720, test:0.009992 | lr:0.000000\n",
            "Epoch[246/300] | loss train:0.002773, test:0.010124 | lr:0.000000\n",
            "Epoch[247/300] | loss train:0.002684, test:0.010012 | lr:0.000000\n",
            "Epoch[248/300] | loss train:0.002728, test:0.010041 | lr:0.000000\n",
            "Epoch[249/300] | loss train:0.002689, test:0.010110 | lr:0.000000\n",
            "Epoch[250/300] | loss train:0.002636, test:0.010036 | lr:0.000000\n",
            "Epoch[251/300] | loss train:0.002694, test:0.010003 | lr:0.000000\n",
            "Epoch[252/300] | loss train:0.002651, test:0.009963 | lr:0.000000\n",
            "Epoch[253/300] | loss train:0.002728, test:0.010058 | lr:0.000000\n",
            "Epoch[254/300] | loss train:0.002717, test:0.009972 | lr:0.000000\n",
            "Epoch[255/300] | loss train:0.002640, test:0.009961 | lr:0.000000\n",
            "Epoch[256/300] | loss train:0.002683, test:0.009973 | lr:0.000000\n",
            "Epoch[257/300] | loss train:0.002634, test:0.010002 | lr:0.000000\n",
            "Epoch[258/300] | loss train:0.002614, test:0.010106 | lr:0.000000\n",
            "Epoch[259/300] | loss train:0.002671, test:0.009950 | lr:0.000000\n",
            "Epoch[260/300] | loss train:0.002634, test:0.010114 | lr:0.000000\n",
            "Epoch[261/300] | loss train:0.002637, test:0.010058 | lr:0.000000\n",
            "Epoch[262/300] | loss train:0.002701, test:0.010037 | lr:0.000000\n",
            "Epoch[263/300] | loss train:0.002739, test:0.009976 | lr:0.000000\n",
            "Epoch[264/300] | loss train:0.002640, test:0.010036 | lr:0.000000\n",
            "Epoch[265/300] | loss train:0.002676, test:0.009985 | lr:0.000000\n",
            "Epoch[266/300] | loss train:0.002743, test:0.010005 | lr:0.000000\n",
            "Epoch[267/300] | loss train:0.002657, test:0.010049 | lr:0.000000\n",
            "Epoch[268/300] | loss train:0.002661, test:0.010075 | lr:0.000000\n",
            "Epoch[269/300] | loss train:0.002714, test:0.010094 | lr:0.000000\n",
            "Epoch[270/300] | loss train:0.002645, test:0.010057 | lr:0.000000\n",
            "Epoch[271/300] | loss train:0.002678, test:0.009989 | lr:0.000000\n",
            "Epoch[272/300] | loss train:0.002553, test:0.010003 | lr:0.000000\n",
            "Epoch[273/300] | loss train:0.002704, test:0.009997 | lr:0.000000\n",
            "Epoch[274/300] | loss train:0.002670, test:0.010021 | lr:0.000000\n",
            "Epoch[275/300] | loss train:0.002746, test:0.009983 | lr:0.000000\n",
            "Epoch[276/300] | loss train:0.002590, test:0.010048 | lr:0.000000\n",
            "Epoch[277/300] | loss train:0.002587, test:0.010138 | lr:0.000000\n",
            "Epoch[278/300] | loss train:0.002605, test:0.010048 | lr:0.000000\n",
            "Epoch[279/300] | loss train:0.002684, test:0.010026 | lr:0.000000\n",
            "Epoch[280/300] | loss train:0.002656, test:0.010034 | lr:0.000000\n",
            "Epoch[281/300] | loss train:0.002688, test:0.010067 | lr:0.000000\n",
            "Epoch[282/300] | loss train:0.002633, test:0.009969 | lr:0.000000\n",
            "Epoch[283/300] | loss train:0.002615, test:0.009984 | lr:0.000000\n",
            "Epoch[284/300] | loss train:0.002697, test:0.010060 | lr:0.000000\n",
            "Epoch[285/300] | loss train:0.002667, test:0.009974 | lr:0.000000\n",
            "Epoch[286/300] | loss train:0.002619, test:0.009956 | lr:0.000000\n",
            "Epoch[287/300] | loss train:0.002661, test:0.009984 | lr:0.000000\n",
            "Epoch[288/300] | loss train:0.002711, test:0.010012 | lr:0.000000\n",
            "Epoch[289/300] | loss train:0.002736, test:0.009989 | lr:0.000000\n",
            "Epoch[290/300] | loss train:0.002666, test:0.010032 | lr:0.000000\n",
            "Epoch[291/300] | loss train:0.002666, test:0.010023 | lr:0.000000\n",
            "Epoch[292/300] | loss train:0.002724, test:0.010033 | lr:0.000000\n",
            "Epoch[293/300] | loss train:0.002617, test:0.010006 | lr:0.000000\n",
            "Epoch[294/300] | loss train:0.002646, test:0.010027 | lr:0.000000\n",
            "Epoch[295/300] | loss train:0.002692, test:0.010031 | lr:0.000000\n",
            "Epoch[296/300] | loss train:0.002637, test:0.010007 | lr:0.000000\n",
            "Epoch[297/300] | loss train:0.002714, test:0.009989 | lr:0.000000\n",
            "Epoch[298/300] | loss train:0.002668, test:0.010026 | lr:0.000000\n",
            "Epoch[299/300] | loss train:0.002677, test:0.010019 | lr:0.000000\n",
            "Epoch[300/300] | loss train:0.002642, test:0.010018 | lr:0.000000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-5bbc851cb4bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    770\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m )  # this is the data type and shape required, [batch, sequence, feature]\n\u001b[0;32m--> 772\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    773\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-5bbc851cb4bf>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;31m# layer 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (10x1 and 8x120)"
          ]
        }
      ]
    }
  ]
}